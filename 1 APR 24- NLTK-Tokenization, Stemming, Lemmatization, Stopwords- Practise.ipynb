{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636dd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d40c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI= \"\"\"Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\n",
    "\n",
    "The potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae7adcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nThe potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6146c872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128d778",
   "metadata": {},
   "source": [
    "#  Tokenization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5e8bc",
   "metadata": {},
   "source": [
    "### 1. Sentence to tokens \n",
    "based on white spaces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cbb551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision',\n",
       " ',',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains',\n",
       " ',',\n",
       " 'including',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'finance',\n",
       " ',',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power',\n",
       " ',',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously',\n",
       " '.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense',\n",
       " ',',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency',\n",
       " ',',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches',\n",
       " ',',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment',\n",
       " '.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "AI_tokens= word_tokenize(AI)\n",
    "AI_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf24fb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648db5e",
   "metadata": {},
   "source": [
    "### 2.  Paragraph to sentences\n",
    "based on full stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4757789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence.',\n",
       " 'From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment.',\n",
       " 'With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.',\n",
       " 'The potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation.',\n",
       " 'However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment.',\n",
       " 'Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "AI_sent= sent_tokenize(AI)\n",
    "AI_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf5a842a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371ef6f",
   "metadata": {},
   "source": [
    "### 3.  Document to paragraph:\n",
    "based on blank line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c83850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.',\n",
       " 'The potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank= blankline_tokenize(AI)\n",
    "AI_blank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341bd345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b08d3c",
   "metadata": {},
   "source": [
    "###  4. Sentences to words:\n",
    "Dont tokenize on  the based on  comma,fullstop , punctuation, regular exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f04212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(AI)',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision,',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains,',\n",
       " 'including',\n",
       " 'healthcare,',\n",
       " 'finance,',\n",
       " 'and',\n",
       " 'entertainment.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power,',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense,',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency,',\n",
       " 'productivity,',\n",
       " 'and',\n",
       " 'innovation.',\n",
       " 'However,',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI,',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches,',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wt= WhitespaceTokenizer()\n",
    "AI_wt= wt.tokenize(AI)\n",
    "AI_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81e04b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e608ff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neha', '2', ';', '!', '#%$!#%*&^E(&).', '<', ',', '>', '>.....', '.;lko;ko']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "word='neha 2 ; ! #%$!#%*&^E(&). < , > >..... .;lko;ko  '\n",
    "wt= WhitespaceTokenizer()\n",
    "AI_wt_word= wt.tokenize(word)\n",
    "AI_wt_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32f321",
   "metadata": {},
   "source": [
    "###  5. Sentences to words:\n",
    " tokenize on  the based on  comma,fullstop , punctuation, regular exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c853ff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision',\n",
       " ',',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains',\n",
       " ',',\n",
       " 'including',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'finance',\n",
       " ',',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power',\n",
       " ',',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously',\n",
       " '.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense',\n",
       " ',',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency',\n",
       " ',',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches',\n",
       " ',',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment',\n",
       " '.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "AI_wpt= wordpunct_tokenize(AI)\n",
    "AI_wpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c77f983a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_wpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8619c",
   "metadata": {},
   "source": [
    "#### The main difference between word tokenization and word punctuate tokenization lies in how they handle punctuation characters.\n",
    "\n",
    " **1. Word Tokenization:**\n",
    "\n",
    "Word tokenization simply splits the text into tokens based on whitespace characters such as spaces, tabs, and newline characters.\n",
    "It does not consider punctuation characters as separate tokens, but rather includes them as part of the adjacent words.\n",
    "WordPunct Tokenization:\n",
    "\n",
    " **2. WordPunct tokenization:**\n",
    "\n",
    "on the other hand, splits the text into tokens based on both whitespace and punctuation characters.\n",
    "It treats punctuation characters as separate tokens, meaning that punctuation marks are tokenized individually as their own tokens.\n",
    "Here's a simple example to illustrate the difference:\n",
    "\n",
    "**Example**\n",
    "Text: \"Hello, world! How are you?\"\n",
    "\n",
    " **Word Tokenization:**\n",
    "Tokens: [\"Hello,\", \"world!\", \"How\", \"are\", \"you?\"]\n",
    "\n",
    "**WordPunct Tokenization:**\n",
    "Tokens: [\"Hello\", \",\", \"world\", \"!\", \"How\", \"are\", \"you\", \"?\"]\n",
    "\n",
    "* In summary, while word tokenization considers only whitespace characters as token boundaries, word punctuate tokenization considers both whitespace and punctuation characters as boundaries, resulting in potentially more tokens, especially when punctuation marks are involved.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14345217",
   "metadata": {},
   "source": [
    "### Diffrence betwwen   \n",
    "### word_tokenize,\n",
    "###                                    wordpunct_tokenize, \n",
    "###                                    whitespaceTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bf0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenize Output: ['Hello', ',', 'world', '!', 'How', ',', 'are', 'you', 'doing', '?', '12', '#', '$', '%', 'z^', ';']\n",
      "Wordpuct Tokenize Output: ['Hello', ',', 'world', '!', 'How', ',', 'are', 'you', 'doing', '?', '12', '#$', '%', 'z', '^', ';']\n",
      "['Hello,world!How,are', 'you', 'doing?', '12#$', '%z^', ';']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, WhitespaceTokenizer\n",
    "\n",
    "text = \"Hello,world!How,are you doing? 12#$ %z^ ;\"\n",
    "\n",
    "word_tokenized = word_tokenize(text)\n",
    "wordpunct_tokenized = wordpunct_tokenize(text)\n",
    "\n",
    "print(\"Word Tokenize Output:\", word_tokenized)\n",
    "print(\"Wordpuct Tokenize Output:\", wordpunct_tokenized)\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "word=\"Hello,world!How,are you doing? 12#$ %z^ ;\"\n",
    "wt= WhitespaceTokenizer()\n",
    "AI_wt_word= wt.tokenize(word)\n",
    "print(AI_wt_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a44d84",
   "metadata": {},
   "source": [
    "#  Types of Tokenizations:\n",
    "### 1. Bigram\n",
    "### 2. Trigram\n",
    "### 3. Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d92b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram => 2 words , # Trigra => 3 words, Ngram=> multi word as per user custom\n",
    "\n",
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530b61e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nThe potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e67788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision',\n",
       " ',',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains',\n",
       " ',',\n",
       " 'including',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'finance',\n",
       " ',',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power',\n",
       " ',',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously',\n",
       " '.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense',\n",
       " ',',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency',\n",
       " ',',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches',\n",
       " ',',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment',\n",
       " '.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_tokens= nltk.word_tokenize(AI)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20862833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quotes_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb02703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'Intelligence'),\n",
       " ('Intelligence', '('),\n",
       " ('(', 'AI'),\n",
       " ('AI', ')'),\n",
       " (')', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'rapidly'),\n",
       " ('rapidly', 'evolving'),\n",
       " ('evolving', 'field'),\n",
       " ('field', 'that'),\n",
       " ('that', 'encompasses'),\n",
       " ('encompasses', 'a'),\n",
       " ('a', 'wide'),\n",
       " ('wide', 'range'),\n",
       " ('range', 'of'),\n",
       " ('of', 'technologies'),\n",
       " ('technologies', 'aimed'),\n",
       " ('aimed', 'at'),\n",
       " ('at', 'mimicking'),\n",
       " ('mimicking', 'human'),\n",
       " ('human', 'intelligence'),\n",
       " ('intelligence', '.'),\n",
       " ('.', 'From'),\n",
       " ('From', 'natural'),\n",
       " ('natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', 'to'),\n",
       " ('to', 'computer'),\n",
       " ('computer', 'vision'),\n",
       " ('vision', ','),\n",
       " (',', 'AI'),\n",
       " ('AI', 'applications'),\n",
       " ('applications', 'span'),\n",
       " ('span', 'various'),\n",
       " ('various', 'domains'),\n",
       " ('domains', ','),\n",
       " (',', 'including'),\n",
       " ('including', 'healthcare'),\n",
       " ('healthcare', ','),\n",
       " (',', 'finance'),\n",
       " ('finance', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'entertainment'),\n",
       " ('entertainment', '.'),\n",
       " ('.', 'With'),\n",
       " ('With', 'advancements'),\n",
       " ('advancements', 'in'),\n",
       " ('in', 'machine'),\n",
       " ('machine', 'learning'),\n",
       " ('learning', 'algorithms'),\n",
       " ('algorithms', 'and'),\n",
       " ('and', 'computational'),\n",
       " ('computational', 'power'),\n",
       " ('power', ','),\n",
       " (',', 'AI'),\n",
       " ('AI', 'systems'),\n",
       " ('systems', 'are'),\n",
       " ('are', 'becoming'),\n",
       " ('becoming', 'increasingly'),\n",
       " ('increasingly', 'capable'),\n",
       " ('capable', 'of'),\n",
       " ('of', 'performing'),\n",
       " ('performing', 'complex'),\n",
       " ('complex', 'tasks'),\n",
       " ('tasks', 'autonomously'),\n",
       " ('autonomously', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'potential'),\n",
       " ('potential', 'of'),\n",
       " ('of', 'AI'),\n",
       " ('AI', 'to'),\n",
       " ('to', 'revolutionize'),\n",
       " ('revolutionize', 'industries'),\n",
       " ('industries', 'and'),\n",
       " ('and', 'streamline'),\n",
       " ('streamline', 'processes'),\n",
       " ('processes', 'is'),\n",
       " ('is', 'immense'),\n",
       " ('immense', ','),\n",
       " (',', 'promising'),\n",
       " ('promising', 'greater'),\n",
       " ('greater', 'efficiency'),\n",
       " ('efficiency', ','),\n",
       " (',', 'productivity'),\n",
       " ('productivity', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'innovation'),\n",
       " ('innovation', '.'),\n",
       " ('.', 'However'),\n",
       " ('However', ','),\n",
       " (',', 'concerns'),\n",
       " ('concerns', 'about'),\n",
       " ('about', 'the'),\n",
       " ('the', 'ethical'),\n",
       " ('ethical', 'implications'),\n",
       " ('implications', 'of'),\n",
       " ('of', 'AI'),\n",
       " ('AI', ','),\n",
       " (',', 'such'),\n",
       " ('such', 'as'),\n",
       " ('as', 'algorithmic'),\n",
       " ('algorithmic', 'bias'),\n",
       " ('bias', 'and'),\n",
       " ('and', 'privacy'),\n",
       " ('privacy', 'breaches'),\n",
       " ('breaches', ','),\n",
       " (',', 'underscore'),\n",
       " ('underscore', 'the'),\n",
       " ('the', 'need'),\n",
       " ('need', 'for'),\n",
       " ('for', 'responsible'),\n",
       " ('responsible', 'development'),\n",
       " ('development', 'and'),\n",
       " ('and', 'deployment'),\n",
       " ('deployment', '.'),\n",
       " ('.', 'Striking'),\n",
       " ('Striking', 'a'),\n",
       " ('a', 'balance'),\n",
       " ('balance', 'between'),\n",
       " ('between', 'harnessing'),\n",
       " ('harnessing', 'the'),\n",
       " ('the', 'benefits'),\n",
       " ('benefits', 'of'),\n",
       " ('of', 'AI'),\n",
       " ('AI', 'and'),\n",
       " ('and', 'addressing'),\n",
       " ('addressing', 'its'),\n",
       " ('its', 'societal'),\n",
       " ('societal', 'impacts'),\n",
       " ('impacts', 'is'),\n",
       " ('is', 'essential'),\n",
       " ('essential', 'for'),\n",
       " ('for', 'shaping'),\n",
       " ('shaping', 'a'),\n",
       " ('a', 'future'),\n",
       " ('future', 'where'),\n",
       " ('where', 'AI'),\n",
       " ('AI', 'serves'),\n",
       " ('serves', 'humanity'),\n",
       " ('humanity', 'best'),\n",
       " ('best', 'interests'),\n",
       " ('interests', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigram = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984def7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'Intelligence', '('),\n",
       " ('Intelligence', '(', 'AI'),\n",
       " ('(', 'AI', ')'),\n",
       " ('AI', ')', 'is'),\n",
       " (')', 'is', 'a'),\n",
       " ('is', 'a', 'rapidly'),\n",
       " ('a', 'rapidly', 'evolving'),\n",
       " ('rapidly', 'evolving', 'field'),\n",
       " ('evolving', 'field', 'that'),\n",
       " ('field', 'that', 'encompasses'),\n",
       " ('that', 'encompasses', 'a'),\n",
       " ('encompasses', 'a', 'wide'),\n",
       " ('a', 'wide', 'range'),\n",
       " ('wide', 'range', 'of'),\n",
       " ('range', 'of', 'technologies'),\n",
       " ('of', 'technologies', 'aimed'),\n",
       " ('technologies', 'aimed', 'at'),\n",
       " ('aimed', 'at', 'mimicking'),\n",
       " ('at', 'mimicking', 'human'),\n",
       " ('mimicking', 'human', 'intelligence'),\n",
       " ('human', 'intelligence', '.'),\n",
       " ('intelligence', '.', 'From'),\n",
       " ('.', 'From', 'natural'),\n",
       " ('From', 'natural', 'language'),\n",
       " ('natural', 'language', 'processing'),\n",
       " ('language', 'processing', 'to'),\n",
       " ('processing', 'to', 'computer'),\n",
       " ('to', 'computer', 'vision'),\n",
       " ('computer', 'vision', ','),\n",
       " ('vision', ',', 'AI'),\n",
       " (',', 'AI', 'applications'),\n",
       " ('AI', 'applications', 'span'),\n",
       " ('applications', 'span', 'various'),\n",
       " ('span', 'various', 'domains'),\n",
       " ('various', 'domains', ','),\n",
       " ('domains', ',', 'including'),\n",
       " (',', 'including', 'healthcare'),\n",
       " ('including', 'healthcare', ','),\n",
       " ('healthcare', ',', 'finance'),\n",
       " (',', 'finance', ','),\n",
       " ('finance', ',', 'and'),\n",
       " (',', 'and', 'entertainment'),\n",
       " ('and', 'entertainment', '.'),\n",
       " ('entertainment', '.', 'With'),\n",
       " ('.', 'With', 'advancements'),\n",
       " ('With', 'advancements', 'in'),\n",
       " ('advancements', 'in', 'machine'),\n",
       " ('in', 'machine', 'learning'),\n",
       " ('machine', 'learning', 'algorithms'),\n",
       " ('learning', 'algorithms', 'and'),\n",
       " ('algorithms', 'and', 'computational'),\n",
       " ('and', 'computational', 'power'),\n",
       " ('computational', 'power', ','),\n",
       " ('power', ',', 'AI'),\n",
       " (',', 'AI', 'systems'),\n",
       " ('AI', 'systems', 'are'),\n",
       " ('systems', 'are', 'becoming'),\n",
       " ('are', 'becoming', 'increasingly'),\n",
       " ('becoming', 'increasingly', 'capable'),\n",
       " ('increasingly', 'capable', 'of'),\n",
       " ('capable', 'of', 'performing'),\n",
       " ('of', 'performing', 'complex'),\n",
       " ('performing', 'complex', 'tasks'),\n",
       " ('complex', 'tasks', 'autonomously'),\n",
       " ('tasks', 'autonomously', '.'),\n",
       " ('autonomously', '.', 'The'),\n",
       " ('.', 'The', 'potential'),\n",
       " ('The', 'potential', 'of'),\n",
       " ('potential', 'of', 'AI'),\n",
       " ('of', 'AI', 'to'),\n",
       " ('AI', 'to', 'revolutionize'),\n",
       " ('to', 'revolutionize', 'industries'),\n",
       " ('revolutionize', 'industries', 'and'),\n",
       " ('industries', 'and', 'streamline'),\n",
       " ('and', 'streamline', 'processes'),\n",
       " ('streamline', 'processes', 'is'),\n",
       " ('processes', 'is', 'immense'),\n",
       " ('is', 'immense', ','),\n",
       " ('immense', ',', 'promising'),\n",
       " (',', 'promising', 'greater'),\n",
       " ('promising', 'greater', 'efficiency'),\n",
       " ('greater', 'efficiency', ','),\n",
       " ('efficiency', ',', 'productivity'),\n",
       " (',', 'productivity', ','),\n",
       " ('productivity', ',', 'and'),\n",
       " (',', 'and', 'innovation'),\n",
       " ('and', 'innovation', '.'),\n",
       " ('innovation', '.', 'However'),\n",
       " ('.', 'However', ','),\n",
       " ('However', ',', 'concerns'),\n",
       " (',', 'concerns', 'about'),\n",
       " ('concerns', 'about', 'the'),\n",
       " ('about', 'the', 'ethical'),\n",
       " ('the', 'ethical', 'implications'),\n",
       " ('ethical', 'implications', 'of'),\n",
       " ('implications', 'of', 'AI'),\n",
       " ('of', 'AI', ','),\n",
       " ('AI', ',', 'such'),\n",
       " (',', 'such', 'as'),\n",
       " ('such', 'as', 'algorithmic'),\n",
       " ('as', 'algorithmic', 'bias'),\n",
       " ('algorithmic', 'bias', 'and'),\n",
       " ('bias', 'and', 'privacy'),\n",
       " ('and', 'privacy', 'breaches'),\n",
       " ('privacy', 'breaches', ','),\n",
       " ('breaches', ',', 'underscore'),\n",
       " (',', 'underscore', 'the'),\n",
       " ('underscore', 'the', 'need'),\n",
       " ('the', 'need', 'for'),\n",
       " ('need', 'for', 'responsible'),\n",
       " ('for', 'responsible', 'development'),\n",
       " ('responsible', 'development', 'and'),\n",
       " ('development', 'and', 'deployment'),\n",
       " ('and', 'deployment', '.'),\n",
       " ('deployment', '.', 'Striking'),\n",
       " ('.', 'Striking', 'a'),\n",
       " ('Striking', 'a', 'balance'),\n",
       " ('a', 'balance', 'between'),\n",
       " ('balance', 'between', 'harnessing'),\n",
       " ('between', 'harnessing', 'the'),\n",
       " ('harnessing', 'the', 'benefits'),\n",
       " ('the', 'benefits', 'of'),\n",
       " ('benefits', 'of', 'AI'),\n",
       " ('of', 'AI', 'and'),\n",
       " ('AI', 'and', 'addressing'),\n",
       " ('and', 'addressing', 'its'),\n",
       " ('addressing', 'its', 'societal'),\n",
       " ('its', 'societal', 'impacts'),\n",
       " ('societal', 'impacts', 'is'),\n",
       " ('impacts', 'is', 'essential'),\n",
       " ('is', 'essential', 'for'),\n",
       " ('essential', 'for', 'shaping'),\n",
       " ('for', 'shaping', 'a'),\n",
       " ('shaping', 'a', 'future'),\n",
       " ('a', 'future', 'where'),\n",
       " ('future', 'where', 'AI'),\n",
       " ('where', 'AI', 'serves'),\n",
       " ('AI', 'serves', 'humanity'),\n",
       " ('serves', 'humanity', 'best'),\n",
       " ('humanity', 'best', 'interests'),\n",
       " ('best', 'interests', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigram = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0665fc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'Intelligence', '(', 'AI', ')', 'is'),\n",
       " ('Intelligence', '(', 'AI', ')', 'is', 'a'),\n",
       " ('(', 'AI', ')', 'is', 'a', 'rapidly'),\n",
       " ('AI', ')', 'is', 'a', 'rapidly', 'evolving'),\n",
       " (')', 'is', 'a', 'rapidly', 'evolving', 'field'),\n",
       " ('is', 'a', 'rapidly', 'evolving', 'field', 'that'),\n",
       " ('a', 'rapidly', 'evolving', 'field', 'that', 'encompasses'),\n",
       " ('rapidly', 'evolving', 'field', 'that', 'encompasses', 'a'),\n",
       " ('evolving', 'field', 'that', 'encompasses', 'a', 'wide'),\n",
       " ('field', 'that', 'encompasses', 'a', 'wide', 'range'),\n",
       " ('that', 'encompasses', 'a', 'wide', 'range', 'of'),\n",
       " ('encompasses', 'a', 'wide', 'range', 'of', 'technologies'),\n",
       " ('a', 'wide', 'range', 'of', 'technologies', 'aimed'),\n",
       " ('wide', 'range', 'of', 'technologies', 'aimed', 'at'),\n",
       " ('range', 'of', 'technologies', 'aimed', 'at', 'mimicking'),\n",
       " ('of', 'technologies', 'aimed', 'at', 'mimicking', 'human'),\n",
       " ('technologies', 'aimed', 'at', 'mimicking', 'human', 'intelligence'),\n",
       " ('aimed', 'at', 'mimicking', 'human', 'intelligence', '.'),\n",
       " ('at', 'mimicking', 'human', 'intelligence', '.', 'From'),\n",
       " ('mimicking', 'human', 'intelligence', '.', 'From', 'natural'),\n",
       " ('human', 'intelligence', '.', 'From', 'natural', 'language'),\n",
       " ('intelligence', '.', 'From', 'natural', 'language', 'processing'),\n",
       " ('.', 'From', 'natural', 'language', 'processing', 'to'),\n",
       " ('From', 'natural', 'language', 'processing', 'to', 'computer'),\n",
       " ('natural', 'language', 'processing', 'to', 'computer', 'vision'),\n",
       " ('language', 'processing', 'to', 'computer', 'vision', ','),\n",
       " ('processing', 'to', 'computer', 'vision', ',', 'AI'),\n",
       " ('to', 'computer', 'vision', ',', 'AI', 'applications'),\n",
       " ('computer', 'vision', ',', 'AI', 'applications', 'span'),\n",
       " ('vision', ',', 'AI', 'applications', 'span', 'various'),\n",
       " (',', 'AI', 'applications', 'span', 'various', 'domains'),\n",
       " ('AI', 'applications', 'span', 'various', 'domains', ','),\n",
       " ('applications', 'span', 'various', 'domains', ',', 'including'),\n",
       " ('span', 'various', 'domains', ',', 'including', 'healthcare'),\n",
       " ('various', 'domains', ',', 'including', 'healthcare', ','),\n",
       " ('domains', ',', 'including', 'healthcare', ',', 'finance'),\n",
       " (',', 'including', 'healthcare', ',', 'finance', ','),\n",
       " ('including', 'healthcare', ',', 'finance', ',', 'and'),\n",
       " ('healthcare', ',', 'finance', ',', 'and', 'entertainment'),\n",
       " (',', 'finance', ',', 'and', 'entertainment', '.'),\n",
       " ('finance', ',', 'and', 'entertainment', '.', 'With'),\n",
       " (',', 'and', 'entertainment', '.', 'With', 'advancements'),\n",
       " ('and', 'entertainment', '.', 'With', 'advancements', 'in'),\n",
       " ('entertainment', '.', 'With', 'advancements', 'in', 'machine'),\n",
       " ('.', 'With', 'advancements', 'in', 'machine', 'learning'),\n",
       " ('With', 'advancements', 'in', 'machine', 'learning', 'algorithms'),\n",
       " ('advancements', 'in', 'machine', 'learning', 'algorithms', 'and'),\n",
       " ('in', 'machine', 'learning', 'algorithms', 'and', 'computational'),\n",
       " ('machine', 'learning', 'algorithms', 'and', 'computational', 'power'),\n",
       " ('learning', 'algorithms', 'and', 'computational', 'power', ','),\n",
       " ('algorithms', 'and', 'computational', 'power', ',', 'AI'),\n",
       " ('and', 'computational', 'power', ',', 'AI', 'systems'),\n",
       " ('computational', 'power', ',', 'AI', 'systems', 'are'),\n",
       " ('power', ',', 'AI', 'systems', 'are', 'becoming'),\n",
       " (',', 'AI', 'systems', 'are', 'becoming', 'increasingly'),\n",
       " ('AI', 'systems', 'are', 'becoming', 'increasingly', 'capable'),\n",
       " ('systems', 'are', 'becoming', 'increasingly', 'capable', 'of'),\n",
       " ('are', 'becoming', 'increasingly', 'capable', 'of', 'performing'),\n",
       " ('becoming', 'increasingly', 'capable', 'of', 'performing', 'complex'),\n",
       " ('increasingly', 'capable', 'of', 'performing', 'complex', 'tasks'),\n",
       " ('capable', 'of', 'performing', 'complex', 'tasks', 'autonomously'),\n",
       " ('of', 'performing', 'complex', 'tasks', 'autonomously', '.'),\n",
       " ('performing', 'complex', 'tasks', 'autonomously', '.', 'The'),\n",
       " ('complex', 'tasks', 'autonomously', '.', 'The', 'potential'),\n",
       " ('tasks', 'autonomously', '.', 'The', 'potential', 'of'),\n",
       " ('autonomously', '.', 'The', 'potential', 'of', 'AI'),\n",
       " ('.', 'The', 'potential', 'of', 'AI', 'to'),\n",
       " ('The', 'potential', 'of', 'AI', 'to', 'revolutionize'),\n",
       " ('potential', 'of', 'AI', 'to', 'revolutionize', 'industries'),\n",
       " ('of', 'AI', 'to', 'revolutionize', 'industries', 'and'),\n",
       " ('AI', 'to', 'revolutionize', 'industries', 'and', 'streamline'),\n",
       " ('to', 'revolutionize', 'industries', 'and', 'streamline', 'processes'),\n",
       " ('revolutionize', 'industries', 'and', 'streamline', 'processes', 'is'),\n",
       " ('industries', 'and', 'streamline', 'processes', 'is', 'immense'),\n",
       " ('and', 'streamline', 'processes', 'is', 'immense', ','),\n",
       " ('streamline', 'processes', 'is', 'immense', ',', 'promising'),\n",
       " ('processes', 'is', 'immense', ',', 'promising', 'greater'),\n",
       " ('is', 'immense', ',', 'promising', 'greater', 'efficiency'),\n",
       " ('immense', ',', 'promising', 'greater', 'efficiency', ','),\n",
       " (',', 'promising', 'greater', 'efficiency', ',', 'productivity'),\n",
       " ('promising', 'greater', 'efficiency', ',', 'productivity', ','),\n",
       " ('greater', 'efficiency', ',', 'productivity', ',', 'and'),\n",
       " ('efficiency', ',', 'productivity', ',', 'and', 'innovation'),\n",
       " (',', 'productivity', ',', 'and', 'innovation', '.'),\n",
       " ('productivity', ',', 'and', 'innovation', '.', 'However'),\n",
       " (',', 'and', 'innovation', '.', 'However', ','),\n",
       " ('and', 'innovation', '.', 'However', ',', 'concerns'),\n",
       " ('innovation', '.', 'However', ',', 'concerns', 'about'),\n",
       " ('.', 'However', ',', 'concerns', 'about', 'the'),\n",
       " ('However', ',', 'concerns', 'about', 'the', 'ethical'),\n",
       " (',', 'concerns', 'about', 'the', 'ethical', 'implications'),\n",
       " ('concerns', 'about', 'the', 'ethical', 'implications', 'of'),\n",
       " ('about', 'the', 'ethical', 'implications', 'of', 'AI'),\n",
       " ('the', 'ethical', 'implications', 'of', 'AI', ','),\n",
       " ('ethical', 'implications', 'of', 'AI', ',', 'such'),\n",
       " ('implications', 'of', 'AI', ',', 'such', 'as'),\n",
       " ('of', 'AI', ',', 'such', 'as', 'algorithmic'),\n",
       " ('AI', ',', 'such', 'as', 'algorithmic', 'bias'),\n",
       " (',', 'such', 'as', 'algorithmic', 'bias', 'and'),\n",
       " ('such', 'as', 'algorithmic', 'bias', 'and', 'privacy'),\n",
       " ('as', 'algorithmic', 'bias', 'and', 'privacy', 'breaches'),\n",
       " ('algorithmic', 'bias', 'and', 'privacy', 'breaches', ','),\n",
       " ('bias', 'and', 'privacy', 'breaches', ',', 'underscore'),\n",
       " ('and', 'privacy', 'breaches', ',', 'underscore', 'the'),\n",
       " ('privacy', 'breaches', ',', 'underscore', 'the', 'need'),\n",
       " ('breaches', ',', 'underscore', 'the', 'need', 'for'),\n",
       " (',', 'underscore', 'the', 'need', 'for', 'responsible'),\n",
       " ('underscore', 'the', 'need', 'for', 'responsible', 'development'),\n",
       " ('the', 'need', 'for', 'responsible', 'development', 'and'),\n",
       " ('need', 'for', 'responsible', 'development', 'and', 'deployment'),\n",
       " ('for', 'responsible', 'development', 'and', 'deployment', '.'),\n",
       " ('responsible', 'development', 'and', 'deployment', '.', 'Striking'),\n",
       " ('development', 'and', 'deployment', '.', 'Striking', 'a'),\n",
       " ('and', 'deployment', '.', 'Striking', 'a', 'balance'),\n",
       " ('deployment', '.', 'Striking', 'a', 'balance', 'between'),\n",
       " ('.', 'Striking', 'a', 'balance', 'between', 'harnessing'),\n",
       " ('Striking', 'a', 'balance', 'between', 'harnessing', 'the'),\n",
       " ('a', 'balance', 'between', 'harnessing', 'the', 'benefits'),\n",
       " ('balance', 'between', 'harnessing', 'the', 'benefits', 'of'),\n",
       " ('between', 'harnessing', 'the', 'benefits', 'of', 'AI'),\n",
       " ('harnessing', 'the', 'benefits', 'of', 'AI', 'and'),\n",
       " ('the', 'benefits', 'of', 'AI', 'and', 'addressing'),\n",
       " ('benefits', 'of', 'AI', 'and', 'addressing', 'its'),\n",
       " ('of', 'AI', 'and', 'addressing', 'its', 'societal'),\n",
       " ('AI', 'and', 'addressing', 'its', 'societal', 'impacts'),\n",
       " ('and', 'addressing', 'its', 'societal', 'impacts', 'is'),\n",
       " ('addressing', 'its', 'societal', 'impacts', 'is', 'essential'),\n",
       " ('its', 'societal', 'impacts', 'is', 'essential', 'for'),\n",
       " ('societal', 'impacts', 'is', 'essential', 'for', 'shaping'),\n",
       " ('impacts', 'is', 'essential', 'for', 'shaping', 'a'),\n",
       " ('is', 'essential', 'for', 'shaping', 'a', 'future'),\n",
       " ('essential', 'for', 'shaping', 'a', 'future', 'where'),\n",
       " ('for', 'shaping', 'a', 'future', 'where', 'AI'),\n",
       " ('shaping', 'a', 'future', 'where', 'AI', 'serves'),\n",
       " ('a', 'future', 'where', 'AI', 'serves', 'humanity'),\n",
       " ('future', 'where', 'AI', 'serves', 'humanity', 'best'),\n",
       " ('where', 'AI', 'serves', 'humanity', 'best', 'interests'),\n",
       " ('AI', 'serves', 'humanity', 'best', 'interests', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_Ngram = list(nltk.ngrams(quotes_tokens, 6))\n",
    "quotes_Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7582ba",
   "metadata": {},
   "source": [
    "# Stemming:\n",
    "\n",
    "### 1. PorterStemmer\n",
    "### 2. LancasterStemmer\n",
    "### 3. SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace9c4a",
   "metadata": {},
   "source": [
    "####  1. PorterStemmer => gives core word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f6d4ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affect'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. PorterStemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "pst= PorterStemmer()\n",
    "\n",
    "pst.stem('affection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fbf5b8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51c9e554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('STemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d2fce51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nThe potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3143d3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence (ai) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. from natural language processing to computer vision, ai applications span various domains, including healthcare, finance, and entertainment. with advancements in machine learning algorithms and computational power, ai systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nthe potential of ai to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. however, concerns about the ethical implications of ai, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. striking a balance between harnessing the benefits of ai and addressing its societal impacts is essential for shaping a future where ai serves humanity best interests.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94674742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('give')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7740fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('giving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "866a5331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neha'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('neha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93c6b07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbdf107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai/*'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('AI/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db6fd8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecfc1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "gaved : gave\n",
      "given : given\n",
      "gave : gave\n",
      "going : go\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + pst.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e88154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "gaved : gave\n",
      "given : given\n",
      "gave : gave\n",
      "going : go\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + pst.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7829949",
   "metadata": {},
   "source": [
    "#### 2. LancasterStemmer => gives ROOT core word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4bdfbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : giv\n",
      "giving : giv\n",
      "gaved : gav\n",
      "given : giv\n",
      "gave : gav\n",
      "going : going\n",
      "gone : gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + lst.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf4e49",
   "metadata": {},
   "source": [
    "#### 3. SnowballStemmer=> acts same like PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4747c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "gaved : gave\n",
      "given : given\n",
      "gave : gave\n",
      "going : go\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snbs = SnowballStemmer(language='english')\n",
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + snbs.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af53775",
   "metadata": {},
   "source": [
    "# Lemmatization:\n",
    "\n",
    "#### Gives full word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb502bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#word_net= wordnet()\n",
    "word_lem= WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34242060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : giving\n",
      "gaved : gaved\n",
      "given : given\n",
      "gave : gave\n",
      "going : going\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + word_lem.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68de30dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giv : giv\n",
      "giv : giv\n",
      "gav : gav\n",
      "giv : giv\n",
      "gav : gav\n",
      "going : going\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['giv', 'giv',  'gav', 'giv', 'gav', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + word_lem.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaf9f3",
   "metadata": {},
   "source": [
    "# StopWords:\n",
    "#### used for text cleaning, data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c408d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0176e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'bcp47.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'extended_omw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw-1.4.zip', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pe08', 'pe08.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet.zip', 'wordnet2021.zip', 'wordnet2022', 'wordnet2022.zip', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0d37add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f05f8008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b6c01c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01',\n",
       " 'ca02',\n",
       " 'ca03',\n",
       " 'ca04',\n",
       " 'ca05',\n",
       " 'ca06',\n",
       " 'ca07',\n",
       " 'ca08',\n",
       " 'ca09',\n",
       " 'ca10',\n",
       " 'ca11',\n",
       " 'ca12',\n",
       " 'ca13',\n",
       " 'ca14',\n",
       " 'ca15',\n",
       " 'ca16',\n",
       " 'ca17',\n",
       " 'ca18',\n",
       " 'ca19',\n",
       " 'ca20',\n",
       " 'ca21',\n",
       " 'ca22',\n",
       " 'ca23',\n",
       " 'ca24',\n",
       " 'ca25',\n",
       " 'ca26',\n",
       " 'ca27',\n",
       " 'ca28',\n",
       " 'ca29',\n",
       " 'ca30',\n",
       " 'ca31',\n",
       " 'ca32',\n",
       " 'ca33',\n",
       " 'ca34',\n",
       " 'ca35',\n",
       " 'ca36',\n",
       " 'ca37',\n",
       " 'ca38',\n",
       " 'ca39',\n",
       " 'ca40',\n",
       " 'ca41',\n",
       " 'ca42',\n",
       " 'ca43',\n",
       " 'ca44',\n",
       " 'cb01',\n",
       " 'cb02',\n",
       " 'cb03',\n",
       " 'cb04',\n",
       " 'cb05',\n",
       " 'cb06',\n",
       " 'cb07',\n",
       " 'cb08',\n",
       " 'cb09',\n",
       " 'cb10',\n",
       " 'cb11',\n",
       " 'cb12',\n",
       " 'cb13',\n",
       " 'cb14',\n",
       " 'cb15',\n",
       " 'cb16',\n",
       " 'cb17',\n",
       " 'cb18',\n",
       " 'cb19',\n",
       " 'cb20',\n",
       " 'cb21',\n",
       " 'cb22',\n",
       " 'cb23',\n",
       " 'cb24',\n",
       " 'cb25',\n",
       " 'cb26',\n",
       " 'cb27',\n",
       " 'cc01',\n",
       " 'cc02',\n",
       " 'cc03',\n",
       " 'cc04',\n",
       " 'cc05',\n",
       " 'cc06',\n",
       " 'cc07',\n",
       " 'cc08',\n",
       " 'cc09',\n",
       " 'cc10',\n",
       " 'cc11',\n",
       " 'cc12',\n",
       " 'cc13',\n",
       " 'cc14',\n",
       " 'cc15',\n",
       " 'cc16',\n",
       " 'cc17',\n",
       " 'cd01',\n",
       " 'cd02',\n",
       " 'cd03',\n",
       " 'cd04',\n",
       " 'cd05',\n",
       " 'cd06',\n",
       " 'cd07',\n",
       " 'cd08',\n",
       " 'cd09',\n",
       " 'cd10',\n",
       " 'cd11',\n",
       " 'cd12',\n",
       " 'cd13',\n",
       " 'cd14',\n",
       " 'cd15',\n",
       " 'cd16',\n",
       " 'cd17',\n",
       " 'ce01',\n",
       " 'ce02',\n",
       " 'ce03',\n",
       " 'ce04',\n",
       " 'ce05',\n",
       " 'ce06',\n",
       " 'ce07',\n",
       " 'ce08',\n",
       " 'ce09',\n",
       " 'ce10',\n",
       " 'ce11',\n",
       " 'ce12',\n",
       " 'ce13',\n",
       " 'ce14',\n",
       " 'ce15',\n",
       " 'ce16',\n",
       " 'ce17',\n",
       " 'ce18',\n",
       " 'ce19',\n",
       " 'ce20',\n",
       " 'ce21',\n",
       " 'ce22',\n",
       " 'ce23',\n",
       " 'ce24',\n",
       " 'ce25',\n",
       " 'ce26',\n",
       " 'ce27',\n",
       " 'ce28',\n",
       " 'ce29',\n",
       " 'ce30',\n",
       " 'ce31',\n",
       " 'ce32',\n",
       " 'ce33',\n",
       " 'ce34',\n",
       " 'ce35',\n",
       " 'ce36',\n",
       " 'cf01',\n",
       " 'cf02',\n",
       " 'cf03',\n",
       " 'cf04',\n",
       " 'cf05',\n",
       " 'cf06',\n",
       " 'cf07',\n",
       " 'cf08',\n",
       " 'cf09',\n",
       " 'cf10',\n",
       " 'cf11',\n",
       " 'cf12',\n",
       " 'cf13',\n",
       " 'cf14',\n",
       " 'cf15',\n",
       " 'cf16',\n",
       " 'cf17',\n",
       " 'cf18',\n",
       " 'cf19',\n",
       " 'cf20',\n",
       " 'cf21',\n",
       " 'cf22',\n",
       " 'cf23',\n",
       " 'cf24',\n",
       " 'cf25',\n",
       " 'cf26',\n",
       " 'cf27',\n",
       " 'cf28',\n",
       " 'cf29',\n",
       " 'cf30',\n",
       " 'cf31',\n",
       " 'cf32',\n",
       " 'cf33',\n",
       " 'cf34',\n",
       " 'cf35',\n",
       " 'cf36',\n",
       " 'cf37',\n",
       " 'cf38',\n",
       " 'cf39',\n",
       " 'cf40',\n",
       " 'cf41',\n",
       " 'cf42',\n",
       " 'cf43',\n",
       " 'cf44',\n",
       " 'cf45',\n",
       " 'cf46',\n",
       " 'cf47',\n",
       " 'cf48',\n",
       " 'cg01',\n",
       " 'cg02',\n",
       " 'cg03',\n",
       " 'cg04',\n",
       " 'cg05',\n",
       " 'cg06',\n",
       " 'cg07',\n",
       " 'cg08',\n",
       " 'cg09',\n",
       " 'cg10',\n",
       " 'cg11',\n",
       " 'cg12',\n",
       " 'cg13',\n",
       " 'cg14',\n",
       " 'cg15',\n",
       " 'cg16',\n",
       " 'cg17',\n",
       " 'cg18',\n",
       " 'cg19',\n",
       " 'cg20',\n",
       " 'cg21',\n",
       " 'cg22',\n",
       " 'cg23',\n",
       " 'cg24',\n",
       " 'cg25',\n",
       " 'cg26',\n",
       " 'cg27',\n",
       " 'cg28',\n",
       " 'cg29',\n",
       " 'cg30',\n",
       " 'cg31',\n",
       " 'cg32',\n",
       " 'cg33',\n",
       " 'cg34',\n",
       " 'cg35',\n",
       " 'cg36',\n",
       " 'cg37',\n",
       " 'cg38',\n",
       " 'cg39',\n",
       " 'cg40',\n",
       " 'cg41',\n",
       " 'cg42',\n",
       " 'cg43',\n",
       " 'cg44',\n",
       " 'cg45',\n",
       " 'cg46',\n",
       " 'cg47',\n",
       " 'cg48',\n",
       " 'cg49',\n",
       " 'cg50',\n",
       " 'cg51',\n",
       " 'cg52',\n",
       " 'cg53',\n",
       " 'cg54',\n",
       " 'cg55',\n",
       " 'cg56',\n",
       " 'cg57',\n",
       " 'cg58',\n",
       " 'cg59',\n",
       " 'cg60',\n",
       " 'cg61',\n",
       " 'cg62',\n",
       " 'cg63',\n",
       " 'cg64',\n",
       " 'cg65',\n",
       " 'cg66',\n",
       " 'cg67',\n",
       " 'cg68',\n",
       " 'cg69',\n",
       " 'cg70',\n",
       " 'cg71',\n",
       " 'cg72',\n",
       " 'cg73',\n",
       " 'cg74',\n",
       " 'cg75',\n",
       " 'ch01',\n",
       " 'ch02',\n",
       " 'ch03',\n",
       " 'ch04',\n",
       " 'ch05',\n",
       " 'ch06',\n",
       " 'ch07',\n",
       " 'ch08',\n",
       " 'ch09',\n",
       " 'ch10',\n",
       " 'ch11',\n",
       " 'ch12',\n",
       " 'ch13',\n",
       " 'ch14',\n",
       " 'ch15',\n",
       " 'ch16',\n",
       " 'ch17',\n",
       " 'ch18',\n",
       " 'ch19',\n",
       " 'ch20',\n",
       " 'ch21',\n",
       " 'ch22',\n",
       " 'ch23',\n",
       " 'ch24',\n",
       " 'ch25',\n",
       " 'ch26',\n",
       " 'ch27',\n",
       " 'ch28',\n",
       " 'ch29',\n",
       " 'ch30',\n",
       " 'cj01',\n",
       " 'cj02',\n",
       " 'cj03',\n",
       " 'cj04',\n",
       " 'cj05',\n",
       " 'cj06',\n",
       " 'cj07',\n",
       " 'cj08',\n",
       " 'cj09',\n",
       " 'cj10',\n",
       " 'cj11',\n",
       " 'cj12',\n",
       " 'cj13',\n",
       " 'cj14',\n",
       " 'cj15',\n",
       " 'cj16',\n",
       " 'cj17',\n",
       " 'cj18',\n",
       " 'cj19',\n",
       " 'cj20',\n",
       " 'cj21',\n",
       " 'cj22',\n",
       " 'cj23',\n",
       " 'cj24',\n",
       " 'cj25',\n",
       " 'cj26',\n",
       " 'cj27',\n",
       " 'cj28',\n",
       " 'cj29',\n",
       " 'cj30',\n",
       " 'cj31',\n",
       " 'cj32',\n",
       " 'cj33',\n",
       " 'cj34',\n",
       " 'cj35',\n",
       " 'cj36',\n",
       " 'cj37',\n",
       " 'cj38',\n",
       " 'cj39',\n",
       " 'cj40',\n",
       " 'cj41',\n",
       " 'cj42',\n",
       " 'cj43',\n",
       " 'cj44',\n",
       " 'cj45',\n",
       " 'cj46',\n",
       " 'cj47',\n",
       " 'cj48',\n",
       " 'cj49',\n",
       " 'cj50',\n",
       " 'cj51',\n",
       " 'cj52',\n",
       " 'cj53',\n",
       " 'cj54',\n",
       " 'cj55',\n",
       " 'cj56',\n",
       " 'cj57',\n",
       " 'cj58',\n",
       " 'cj59',\n",
       " 'cj60',\n",
       " 'cj61',\n",
       " 'cj62',\n",
       " 'cj63',\n",
       " 'cj64',\n",
       " 'cj65',\n",
       " 'cj66',\n",
       " 'cj67',\n",
       " 'cj68',\n",
       " 'cj69',\n",
       " 'cj70',\n",
       " 'cj71',\n",
       " 'cj72',\n",
       " 'cj73',\n",
       " 'cj74',\n",
       " 'cj75',\n",
       " 'cj76',\n",
       " 'cj77',\n",
       " 'cj78',\n",
       " 'cj79',\n",
       " 'cj80',\n",
       " 'ck01',\n",
       " 'ck02',\n",
       " 'ck03',\n",
       " 'ck04',\n",
       " 'ck05',\n",
       " 'ck06',\n",
       " 'ck07',\n",
       " 'ck08',\n",
       " 'ck09',\n",
       " 'ck10',\n",
       " 'ck11',\n",
       " 'ck12',\n",
       " 'ck13',\n",
       " 'ck14',\n",
       " 'ck15',\n",
       " 'ck16',\n",
       " 'ck17',\n",
       " 'ck18',\n",
       " 'ck19',\n",
       " 'ck20',\n",
       " 'ck21',\n",
       " 'ck22',\n",
       " 'ck23',\n",
       " 'ck24',\n",
       " 'ck25',\n",
       " 'ck26',\n",
       " 'ck27',\n",
       " 'ck28',\n",
       " 'ck29',\n",
       " 'cl01',\n",
       " 'cl02',\n",
       " 'cl03',\n",
       " 'cl04',\n",
       " 'cl05',\n",
       " 'cl06',\n",
       " 'cl07',\n",
       " 'cl08',\n",
       " 'cl09',\n",
       " 'cl10',\n",
       " 'cl11',\n",
       " 'cl12',\n",
       " 'cl13',\n",
       " 'cl14',\n",
       " 'cl15',\n",
       " 'cl16',\n",
       " 'cl17',\n",
       " 'cl18',\n",
       " 'cl19',\n",
       " 'cl20',\n",
       " 'cl21',\n",
       " 'cl22',\n",
       " 'cl23',\n",
       " 'cl24',\n",
       " 'cm01',\n",
       " 'cm02',\n",
       " 'cm03',\n",
       " 'cm04',\n",
       " 'cm05',\n",
       " 'cm06',\n",
       " 'cn01',\n",
       " 'cn02',\n",
       " 'cn03',\n",
       " 'cn04',\n",
       " 'cn05',\n",
       " 'cn06',\n",
       " 'cn07',\n",
       " 'cn08',\n",
       " 'cn09',\n",
       " 'cn10',\n",
       " 'cn11',\n",
       " 'cn12',\n",
       " 'cn13',\n",
       " 'cn14',\n",
       " 'cn15',\n",
       " 'cn16',\n",
       " 'cn17',\n",
       " 'cn18',\n",
       " 'cn19',\n",
       " 'cn20',\n",
       " 'cn21',\n",
       " 'cn22',\n",
       " 'cn23',\n",
       " 'cn24',\n",
       " 'cn25',\n",
       " 'cn26',\n",
       " 'cn27',\n",
       " 'cn28',\n",
       " 'cn29',\n",
       " 'cp01',\n",
       " 'cp02',\n",
       " 'cp03',\n",
       " 'cp04',\n",
       " 'cp05',\n",
       " 'cp06',\n",
       " 'cp07',\n",
       " 'cp08',\n",
       " 'cp09',\n",
       " 'cp10',\n",
       " 'cp11',\n",
       " 'cp12',\n",
       " 'cp13',\n",
       " 'cp14',\n",
       " 'cp15',\n",
       " 'cp16',\n",
       " 'cp17',\n",
       " 'cp18',\n",
       " 'cp19',\n",
       " 'cp20',\n",
       " 'cp21',\n",
       " 'cp22',\n",
       " 'cp23',\n",
       " 'cp24',\n",
       " 'cp25',\n",
       " 'cp26',\n",
       " 'cp27',\n",
       " 'cp28',\n",
       " 'cp29',\n",
       " 'cr01',\n",
       " 'cr02',\n",
       " 'cr03',\n",
       " 'cr04',\n",
       " 'cr05',\n",
       " 'cr06',\n",
       " 'cr07',\n",
       " 'cr08',\n",
       " 'cr09']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6eb5ea0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8613746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001375de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "833cb05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84c89564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'da',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'fr',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'knnen',\n",
       " 'knnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'ber',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'whrend',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'wrde',\n",
       " 'wrden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ebb5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\Neha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\hindi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhindi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39mjoin(file)\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\Neha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\hindi'"
     ]
    }
   ],
   "source": [
    "stopwords.words('hindi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e18a88",
   "metadata": {},
   "source": [
    "# As we see there is no inbuilt stopwords lib for hindi but  I have made a project for hindi stopwords also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030b90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812723b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
