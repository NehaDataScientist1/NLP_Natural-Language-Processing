{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636dd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d40c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI= \"\"\"Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\n",
    "\n",
    "The potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae7adcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nThe potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6146c872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128d778",
   "metadata": {},
   "source": [
    "#  Tokenization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f5e8bc",
   "metadata": {},
   "source": [
    "### 1. Sentence to tokens \n",
    "based on white spaces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cbb551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision',\n",
       " ',',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains',\n",
       " ',',\n",
       " 'including',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'finance',\n",
       " ',',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power',\n",
       " ',',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously',\n",
       " '.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense',\n",
       " ',',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency',\n",
       " ',',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches',\n",
       " ',',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment',\n",
       " '.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "AI_tokens= word_tokenize(AI)\n",
    "AI_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf24fb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648db5e",
   "metadata": {},
   "source": [
    "### 2.  Paragraph to sentences\n",
    "based on full stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4757789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence.',\n",
       " 'From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment.',\n",
       " 'With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.',\n",
       " 'The potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation.',\n",
       " 'However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment.',\n",
       " 'Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "AI_sent= sent_tokenize(AI)\n",
    "AI_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf5a842a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371ef6f",
   "metadata": {},
   "source": [
    "### 3.  Document to paragraph:\n",
    "based on blank line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c83850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.',\n",
       " 'The potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank= blankline_tokenize(AI)\n",
    "AI_blank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341bd345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b08d3c",
   "metadata": {},
   "source": [
    "###  4. Sentences to words:\n",
    "Dont tokenize on  the based on  comma,fullstop , punctuation, regular exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f04212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(AI)',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision,',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains,',\n",
       " 'including',\n",
       " 'healthcare,',\n",
       " 'finance,',\n",
       " 'and',\n",
       " 'entertainment.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power,',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense,',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency,',\n",
       " 'productivity,',\n",
       " 'and',\n",
       " 'innovation.',\n",
       " 'However,',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI,',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches,',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wt= WhitespaceTokenizer()\n",
    "AI_wt= wt.tokenize(AI)\n",
    "AI_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81e04b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e608ff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neha', '2', ';', '!', '#%$!#%*&^E(&).', '<', ',', '>', '>.....', '.;lko;ko']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "word='neha 2 ; ! #%$!#%*&^E(&). < , > >..... .;lko;ko  '\n",
    "wt= WhitespaceTokenizer()\n",
    "AI_wt_word= wt.tokenize(word)\n",
    "AI_wt_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32f321",
   "metadata": {},
   "source": [
    "###  5. Sentences to words:\n",
    " tokenize on  the based on  comma,fullstop , punctuation, regular exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c853ff99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision',\n",
       " ',',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains',\n",
       " ',',\n",
       " 'including',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'finance',\n",
       " ',',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power',\n",
       " ',',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously',\n",
       " '.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense',\n",
       " ',',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency',\n",
       " ',',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches',\n",
       " ',',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment',\n",
       " '.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "AI_wpt= wordpunct_tokenize(AI)\n",
    "AI_wpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c77f983a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_wpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e8619c",
   "metadata": {},
   "source": [
    "#### The main difference between word tokenization and word punctuate tokenization lies in how they handle punctuation characters.\n",
    "\n",
    " **1. Word Tokenization:**\n",
    "\n",
    "Word tokenization simply splits the text into tokens based on whitespace characters such as spaces, tabs, and newline characters.\n",
    "It does not consider punctuation characters as separate tokens, but rather includes them as part of the adjacent words.\n",
    "WordPunct Tokenization:\n",
    "\n",
    " **2. WordPunct tokenization:**\n",
    "\n",
    "on the other hand, splits the text into tokens based on both whitespace and punctuation characters.\n",
    "It treats punctuation characters as separate tokens, meaning that punctuation marks are tokenized individually as their own tokens.\n",
    "Here's a simple example to illustrate the difference:\n",
    "\n",
    "**Example**\n",
    "Text: \"Hello, world! How are you?\"\n",
    "\n",
    " **Word Tokenization:**\n",
    "Tokens: [\"Hello,\", \"world!\", \"How\", \"are\", \"you?\"]\n",
    "\n",
    "**WordPunct Tokenization:**\n",
    "Tokens: [\"Hello\", \",\", \"world\", \"!\", \"How\", \"are\", \"you\", \"?\"]\n",
    "\n",
    "* In summary, while word tokenization considers only whitespace characters as token boundaries, word punctuate tokenization considers both whitespace and punctuation characters as boundaries, resulting in potentially more tokens, especially when punctuation marks are involved.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14345217",
   "metadata": {},
   "source": [
    "### Diffrence betwwen   \n",
    "### word_tokenize,\n",
    "###                                    wordpunct_tokenize, \n",
    "###                                    whitespaceTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6bf0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenize Output: ['Hello', ',', 'world', '!', 'How', ',', 'are', 'you', 'doing', '?', '12', '#', '$', '%', 'z^', ';']\n",
      "Wordpuct Tokenize Output: ['Hello', ',', 'world', '!', 'How', ',', 'are', 'you', 'doing', '?', '12', '#$', '%', 'z', '^', ';']\n",
      "['Hello,world!How,are', 'you', 'doing?', '12#$', '%z^', ';']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, WhitespaceTokenizer\n",
    "\n",
    "text = \"Hello,world!How,are you doing? 12#$ %z^ ;\"\n",
    "\n",
    "word_tokenized = word_tokenize(text)\n",
    "wordpunct_tokenized = wordpunct_tokenize(text)\n",
    "\n",
    "print(\"Word Tokenize Output:\", word_tokenized)\n",
    "print(\"Wordpuct Tokenize Output:\", wordpunct_tokenized)\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "word=\"Hello,world!How,are you doing? 12#$ %z^ ;\"\n",
    "wt= WhitespaceTokenizer()\n",
    "AI_wt_word= wt.tokenize(word)\n",
    "print(AI_wt_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a44d84",
   "metadata": {},
   "source": [
    "#  Types of Tokenizations:\n",
    "### 1. Bigram\n",
    "### 2. Trigram\n",
    "### 3. Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d92b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram => 2 words , # Trigra => 3 words, Ngram=> multi word as per user custom\n",
    "\n",
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530b61e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nThe potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e67788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'Intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'field',\n",
       " 'that',\n",
       " 'encompasses',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'technologies',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'mimicking',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'From',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision',\n",
       " ',',\n",
       " 'AI',\n",
       " 'applications',\n",
       " 'span',\n",
       " 'various',\n",
       " 'domains',\n",
       " ',',\n",
       " 'including',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'finance',\n",
       " ',',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'With',\n",
       " 'advancements',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'power',\n",
       " ',',\n",
       " 'AI',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'tasks',\n",
       " 'autonomously',\n",
       " '.',\n",
       " 'The',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'revolutionize',\n",
       " 'industries',\n",
       " 'and',\n",
       " 'streamline',\n",
       " 'processes',\n",
       " 'is',\n",
       " 'immense',\n",
       " ',',\n",
       " 'promising',\n",
       " 'greater',\n",
       " 'efficiency',\n",
       " ',',\n",
       " 'productivity',\n",
       " ',',\n",
       " 'and',\n",
       " 'innovation',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'concerns',\n",
       " 'about',\n",
       " 'the',\n",
       " 'ethical',\n",
       " 'implications',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'algorithmic',\n",
       " 'bias',\n",
       " 'and',\n",
       " 'privacy',\n",
       " 'breaches',\n",
       " ',',\n",
       " 'underscore',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'responsible',\n",
       " 'development',\n",
       " 'and',\n",
       " 'deployment',\n",
       " '.',\n",
       " 'Striking',\n",
       " 'a',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'harnessing',\n",
       " 'the',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'its',\n",
       " 'societal',\n",
       " 'impacts',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'for',\n",
       " 'shaping',\n",
       " 'a',\n",
       " 'future',\n",
       " 'where',\n",
       " 'AI',\n",
       " 'serves',\n",
       " 'humanity',\n",
       " 'best',\n",
       " 'interests',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_tokens= nltk.word_tokenize(AI)\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20862833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quotes_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb02703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'Intelligence'),\n",
       " ('Intelligence', '('),\n",
       " ('(', 'AI'),\n",
       " ('AI', ')'),\n",
       " (')', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'rapidly'),\n",
       " ('rapidly', 'evolving'),\n",
       " ('evolving', 'field'),\n",
       " ('field', 'that'),\n",
       " ('that', 'encompasses'),\n",
       " ('encompasses', 'a'),\n",
       " ('a', 'wide'),\n",
       " ('wide', 'range'),\n",
       " ('range', 'of'),\n",
       " ('of', 'technologies'),\n",
       " ('technologies', 'aimed'),\n",
       " ('aimed', 'at'),\n",
       " ('at', 'mimicking'),\n",
       " ('mimicking', 'human'),\n",
       " ('human', 'intelligence'),\n",
       " ('intelligence', '.'),\n",
       " ('.', 'From'),\n",
       " ('From', 'natural'),\n",
       " ('natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', 'to'),\n",
       " ('to', 'computer'),\n",
       " ('computer', 'vision'),\n",
       " ('vision', ','),\n",
       " (',', 'AI'),\n",
       " ('AI', 'applications'),\n",
       " ('applications', 'span'),\n",
       " ('span', 'various'),\n",
       " ('various', 'domains'),\n",
       " ('domains', ','),\n",
       " (',', 'including'),\n",
       " ('including', 'healthcare'),\n",
       " ('healthcare', ','),\n",
       " (',', 'finance'),\n",
       " ('finance', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'entertainment'),\n",
       " ('entertainment', '.'),\n",
       " ('.', 'With'),\n",
       " ('With', 'advancements'),\n",
       " ('advancements', 'in'),\n",
       " ('in', 'machine'),\n",
       " ('machine', 'learning'),\n",
       " ('learning', 'algorithms'),\n",
       " ('algorithms', 'and'),\n",
       " ('and', 'computational'),\n",
       " ('computational', 'power'),\n",
       " ('power', ','),\n",
       " (',', 'AI'),\n",
       " ('AI', 'systems'),\n",
       " ('systems', 'are'),\n",
       " ('are', 'becoming'),\n",
       " ('becoming', 'increasingly'),\n",
       " ('increasingly', 'capable'),\n",
       " ('capable', 'of'),\n",
       " ('of', 'performing'),\n",
       " ('performing', 'complex'),\n",
       " ('complex', 'tasks'),\n",
       " ('tasks', 'autonomously'),\n",
       " ('autonomously', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'potential'),\n",
       " ('potential', 'of'),\n",
       " ('of', 'AI'),\n",
       " ('AI', 'to'),\n",
       " ('to', 'revolutionize'),\n",
       " ('revolutionize', 'industries'),\n",
       " ('industries', 'and'),\n",
       " ('and', 'streamline'),\n",
       " ('streamline', 'processes'),\n",
       " ('processes', 'is'),\n",
       " ('is', 'immense'),\n",
       " ('immense', ','),\n",
       " (',', 'promising'),\n",
       " ('promising', 'greater'),\n",
       " ('greater', 'efficiency'),\n",
       " ('efficiency', ','),\n",
       " (',', 'productivity'),\n",
       " ('productivity', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'innovation'),\n",
       " ('innovation', '.'),\n",
       " ('.', 'However'),\n",
       " ('However', ','),\n",
       " (',', 'concerns'),\n",
       " ('concerns', 'about'),\n",
       " ('about', 'the'),\n",
       " ('the', 'ethical'),\n",
       " ('ethical', 'implications'),\n",
       " ('implications', 'of'),\n",
       " ('of', 'AI'),\n",
       " ('AI', ','),\n",
       " (',', 'such'),\n",
       " ('such', 'as'),\n",
       " ('as', 'algorithmic'),\n",
       " ('algorithmic', 'bias'),\n",
       " ('bias', 'and'),\n",
       " ('and', 'privacy'),\n",
       " ('privacy', 'breaches'),\n",
       " ('breaches', ','),\n",
       " (',', 'underscore'),\n",
       " ('underscore', 'the'),\n",
       " ('the', 'need'),\n",
       " ('need', 'for'),\n",
       " ('for', 'responsible'),\n",
       " ('responsible', 'development'),\n",
       " ('development', 'and'),\n",
       " ('and', 'deployment'),\n",
       " ('deployment', '.'),\n",
       " ('.', 'Striking'),\n",
       " ('Striking', 'a'),\n",
       " ('a', 'balance'),\n",
       " ('balance', 'between'),\n",
       " ('between', 'harnessing'),\n",
       " ('harnessing', 'the'),\n",
       " ('the', 'benefits'),\n",
       " ('benefits', 'of'),\n",
       " ('of', 'AI'),\n",
       " ('AI', 'and'),\n",
       " ('and', 'addressing'),\n",
       " ('addressing', 'its'),\n",
       " ('its', 'societal'),\n",
       " ('societal', 'impacts'),\n",
       " ('impacts', 'is'),\n",
       " ('is', 'essential'),\n",
       " ('essential', 'for'),\n",
       " ('for', 'shaping'),\n",
       " ('shaping', 'a'),\n",
       " ('a', 'future'),\n",
       " ('future', 'where'),\n",
       " ('where', 'AI'),\n",
       " ('AI', 'serves'),\n",
       " ('serves', 'humanity'),\n",
       " ('humanity', 'best'),\n",
       " ('best', 'interests'),\n",
       " ('interests', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigram = list(nltk.bigrams(quotes_tokens))\n",
    "quotes_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984def7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'Intelligence', '('),\n",
       " ('Intelligence', '(', 'AI'),\n",
       " ('(', 'AI', ')'),\n",
       " ('AI', ')', 'is'),\n",
       " (')', 'is', 'a'),\n",
       " ('is', 'a', 'rapidly'),\n",
       " ('a', 'rapidly', 'evolving'),\n",
       " ('rapidly', 'evolving', 'field'),\n",
       " ('evolving', 'field', 'that'),\n",
       " ('field', 'that', 'encompasses'),\n",
       " ('that', 'encompasses', 'a'),\n",
       " ('encompasses', 'a', 'wide'),\n",
       " ('a', 'wide', 'range'),\n",
       " ('wide', 'range', 'of'),\n",
       " ('range', 'of', 'technologies'),\n",
       " ('of', 'technologies', 'aimed'),\n",
       " ('technologies', 'aimed', 'at'),\n",
       " ('aimed', 'at', 'mimicking'),\n",
       " ('at', 'mimicking', 'human'),\n",
       " ('mimicking', 'human', 'intelligence'),\n",
       " ('human', 'intelligence', '.'),\n",
       " ('intelligence', '.', 'From'),\n",
       " ('.', 'From', 'natural'),\n",
       " ('From', 'natural', 'language'),\n",
       " ('natural', 'language', 'processing'),\n",
       " ('language', 'processing', 'to'),\n",
       " ('processing', 'to', 'computer'),\n",
       " ('to', 'computer', 'vision'),\n",
       " ('computer', 'vision', ','),\n",
       " ('vision', ',', 'AI'),\n",
       " (',', 'AI', 'applications'),\n",
       " ('AI', 'applications', 'span'),\n",
       " ('applications', 'span', 'various'),\n",
       " ('span', 'various', 'domains'),\n",
       " ('various', 'domains', ','),\n",
       " ('domains', ',', 'including'),\n",
       " (',', 'including', 'healthcare'),\n",
       " ('including', 'healthcare', ','),\n",
       " ('healthcare', ',', 'finance'),\n",
       " (',', 'finance', ','),\n",
       " ('finance', ',', 'and'),\n",
       " (',', 'and', 'entertainment'),\n",
       " ('and', 'entertainment', '.'),\n",
       " ('entertainment', '.', 'With'),\n",
       " ('.', 'With', 'advancements'),\n",
       " ('With', 'advancements', 'in'),\n",
       " ('advancements', 'in', 'machine'),\n",
       " ('in', 'machine', 'learning'),\n",
       " ('machine', 'learning', 'algorithms'),\n",
       " ('learning', 'algorithms', 'and'),\n",
       " ('algorithms', 'and', 'computational'),\n",
       " ('and', 'computational', 'power'),\n",
       " ('computational', 'power', ','),\n",
       " ('power', ',', 'AI'),\n",
       " (',', 'AI', 'systems'),\n",
       " ('AI', 'systems', 'are'),\n",
       " ('systems', 'are', 'becoming'),\n",
       " ('are', 'becoming', 'increasingly'),\n",
       " ('becoming', 'increasingly', 'capable'),\n",
       " ('increasingly', 'capable', 'of'),\n",
       " ('capable', 'of', 'performing'),\n",
       " ('of', 'performing', 'complex'),\n",
       " ('performing', 'complex', 'tasks'),\n",
       " ('complex', 'tasks', 'autonomously'),\n",
       " ('tasks', 'autonomously', '.'),\n",
       " ('autonomously', '.', 'The'),\n",
       " ('.', 'The', 'potential'),\n",
       " ('The', 'potential', 'of'),\n",
       " ('potential', 'of', 'AI'),\n",
       " ('of', 'AI', 'to'),\n",
       " ('AI', 'to', 'revolutionize'),\n",
       " ('to', 'revolutionize', 'industries'),\n",
       " ('revolutionize', 'industries', 'and'),\n",
       " ('industries', 'and', 'streamline'),\n",
       " ('and', 'streamline', 'processes'),\n",
       " ('streamline', 'processes', 'is'),\n",
       " ('processes', 'is', 'immense'),\n",
       " ('is', 'immense', ','),\n",
       " ('immense', ',', 'promising'),\n",
       " (',', 'promising', 'greater'),\n",
       " ('promising', 'greater', 'efficiency'),\n",
       " ('greater', 'efficiency', ','),\n",
       " ('efficiency', ',', 'productivity'),\n",
       " (',', 'productivity', ','),\n",
       " ('productivity', ',', 'and'),\n",
       " (',', 'and', 'innovation'),\n",
       " ('and', 'innovation', '.'),\n",
       " ('innovation', '.', 'However'),\n",
       " ('.', 'However', ','),\n",
       " ('However', ',', 'concerns'),\n",
       " (',', 'concerns', 'about'),\n",
       " ('concerns', 'about', 'the'),\n",
       " ('about', 'the', 'ethical'),\n",
       " ('the', 'ethical', 'implications'),\n",
       " ('ethical', 'implications', 'of'),\n",
       " ('implications', 'of', 'AI'),\n",
       " ('of', 'AI', ','),\n",
       " ('AI', ',', 'such'),\n",
       " (',', 'such', 'as'),\n",
       " ('such', 'as', 'algorithmic'),\n",
       " ('as', 'algorithmic', 'bias'),\n",
       " ('algorithmic', 'bias', 'and'),\n",
       " ('bias', 'and', 'privacy'),\n",
       " ('and', 'privacy', 'breaches'),\n",
       " ('privacy', 'breaches', ','),\n",
       " ('breaches', ',', 'underscore'),\n",
       " (',', 'underscore', 'the'),\n",
       " ('underscore', 'the', 'need'),\n",
       " ('the', 'need', 'for'),\n",
       " ('need', 'for', 'responsible'),\n",
       " ('for', 'responsible', 'development'),\n",
       " ('responsible', 'development', 'and'),\n",
       " ('development', 'and', 'deployment'),\n",
       " ('and', 'deployment', '.'),\n",
       " ('deployment', '.', 'Striking'),\n",
       " ('.', 'Striking', 'a'),\n",
       " ('Striking', 'a', 'balance'),\n",
       " ('a', 'balance', 'between'),\n",
       " ('balance', 'between', 'harnessing'),\n",
       " ('between', 'harnessing', 'the'),\n",
       " ('harnessing', 'the', 'benefits'),\n",
       " ('the', 'benefits', 'of'),\n",
       " ('benefits', 'of', 'AI'),\n",
       " ('of', 'AI', 'and'),\n",
       " ('AI', 'and', 'addressing'),\n",
       " ('and', 'addressing', 'its'),\n",
       " ('addressing', 'its', 'societal'),\n",
       " ('its', 'societal', 'impacts'),\n",
       " ('societal', 'impacts', 'is'),\n",
       " ('impacts', 'is', 'essential'),\n",
       " ('is', 'essential', 'for'),\n",
       " ('essential', 'for', 'shaping'),\n",
       " ('for', 'shaping', 'a'),\n",
       " ('shaping', 'a', 'future'),\n",
       " ('a', 'future', 'where'),\n",
       " ('future', 'where', 'AI'),\n",
       " ('where', 'AI', 'serves'),\n",
       " ('AI', 'serves', 'humanity'),\n",
       " ('serves', 'humanity', 'best'),\n",
       " ('humanity', 'best', 'interests'),\n",
       " ('best', 'interests', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigram = list(nltk.trigrams(quotes_tokens))\n",
    "quotes_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0665fc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'Intelligence', '(', 'AI', ')', 'is'),\n",
       " ('Intelligence', '(', 'AI', ')', 'is', 'a'),\n",
       " ('(', 'AI', ')', 'is', 'a', 'rapidly'),\n",
       " ('AI', ')', 'is', 'a', 'rapidly', 'evolving'),\n",
       " (')', 'is', 'a', 'rapidly', 'evolving', 'field'),\n",
       " ('is', 'a', 'rapidly', 'evolving', 'field', 'that'),\n",
       " ('a', 'rapidly', 'evolving', 'field', 'that', 'encompasses'),\n",
       " ('rapidly', 'evolving', 'field', 'that', 'encompasses', 'a'),\n",
       " ('evolving', 'field', 'that', 'encompasses', 'a', 'wide'),\n",
       " ('field', 'that', 'encompasses', 'a', 'wide', 'range'),\n",
       " ('that', 'encompasses', 'a', 'wide', 'range', 'of'),\n",
       " ('encompasses', 'a', 'wide', 'range', 'of', 'technologies'),\n",
       " ('a', 'wide', 'range', 'of', 'technologies', 'aimed'),\n",
       " ('wide', 'range', 'of', 'technologies', 'aimed', 'at'),\n",
       " ('range', 'of', 'technologies', 'aimed', 'at', 'mimicking'),\n",
       " ('of', 'technologies', 'aimed', 'at', 'mimicking', 'human'),\n",
       " ('technologies', 'aimed', 'at', 'mimicking', 'human', 'intelligence'),\n",
       " ('aimed', 'at', 'mimicking', 'human', 'intelligence', '.'),\n",
       " ('at', 'mimicking', 'human', 'intelligence', '.', 'From'),\n",
       " ('mimicking', 'human', 'intelligence', '.', 'From', 'natural'),\n",
       " ('human', 'intelligence', '.', 'From', 'natural', 'language'),\n",
       " ('intelligence', '.', 'From', 'natural', 'language', 'processing'),\n",
       " ('.', 'From', 'natural', 'language', 'processing', 'to'),\n",
       " ('From', 'natural', 'language', 'processing', 'to', 'computer'),\n",
       " ('natural', 'language', 'processing', 'to', 'computer', 'vision'),\n",
       " ('language', 'processing', 'to', 'computer', 'vision', ','),\n",
       " ('processing', 'to', 'computer', 'vision', ',', 'AI'),\n",
       " ('to', 'computer', 'vision', ',', 'AI', 'applications'),\n",
       " ('computer', 'vision', ',', 'AI', 'applications', 'span'),\n",
       " ('vision', ',', 'AI', 'applications', 'span', 'various'),\n",
       " (',', 'AI', 'applications', 'span', 'various', 'domains'),\n",
       " ('AI', 'applications', 'span', 'various', 'domains', ','),\n",
       " ('applications', 'span', 'various', 'domains', ',', 'including'),\n",
       " ('span', 'various', 'domains', ',', 'including', 'healthcare'),\n",
       " ('various', 'domains', ',', 'including', 'healthcare', ','),\n",
       " ('domains', ',', 'including', 'healthcare', ',', 'finance'),\n",
       " (',', 'including', 'healthcare', ',', 'finance', ','),\n",
       " ('including', 'healthcare', ',', 'finance', ',', 'and'),\n",
       " ('healthcare', ',', 'finance', ',', 'and', 'entertainment'),\n",
       " (',', 'finance', ',', 'and', 'entertainment', '.'),\n",
       " ('finance', ',', 'and', 'entertainment', '.', 'With'),\n",
       " (',', 'and', 'entertainment', '.', 'With', 'advancements'),\n",
       " ('and', 'entertainment', '.', 'With', 'advancements', 'in'),\n",
       " ('entertainment', '.', 'With', 'advancements', 'in', 'machine'),\n",
       " ('.', 'With', 'advancements', 'in', 'machine', 'learning'),\n",
       " ('With', 'advancements', 'in', 'machine', 'learning', 'algorithms'),\n",
       " ('advancements', 'in', 'machine', 'learning', 'algorithms', 'and'),\n",
       " ('in', 'machine', 'learning', 'algorithms', 'and', 'computational'),\n",
       " ('machine', 'learning', 'algorithms', 'and', 'computational', 'power'),\n",
       " ('learning', 'algorithms', 'and', 'computational', 'power', ','),\n",
       " ('algorithms', 'and', 'computational', 'power', ',', 'AI'),\n",
       " ('and', 'computational', 'power', ',', 'AI', 'systems'),\n",
       " ('computational', 'power', ',', 'AI', 'systems', 'are'),\n",
       " ('power', ',', 'AI', 'systems', 'are', 'becoming'),\n",
       " (',', 'AI', 'systems', 'are', 'becoming', 'increasingly'),\n",
       " ('AI', 'systems', 'are', 'becoming', 'increasingly', 'capable'),\n",
       " ('systems', 'are', 'becoming', 'increasingly', 'capable', 'of'),\n",
       " ('are', 'becoming', 'increasingly', 'capable', 'of', 'performing'),\n",
       " ('becoming', 'increasingly', 'capable', 'of', 'performing', 'complex'),\n",
       " ('increasingly', 'capable', 'of', 'performing', 'complex', 'tasks'),\n",
       " ('capable', 'of', 'performing', 'complex', 'tasks', 'autonomously'),\n",
       " ('of', 'performing', 'complex', 'tasks', 'autonomously', '.'),\n",
       " ('performing', 'complex', 'tasks', 'autonomously', '.', 'The'),\n",
       " ('complex', 'tasks', 'autonomously', '.', 'The', 'potential'),\n",
       " ('tasks', 'autonomously', '.', 'The', 'potential', 'of'),\n",
       " ('autonomously', '.', 'The', 'potential', 'of', 'AI'),\n",
       " ('.', 'The', 'potential', 'of', 'AI', 'to'),\n",
       " ('The', 'potential', 'of', 'AI', 'to', 'revolutionize'),\n",
       " ('potential', 'of', 'AI', 'to', 'revolutionize', 'industries'),\n",
       " ('of', 'AI', 'to', 'revolutionize', 'industries', 'and'),\n",
       " ('AI', 'to', 'revolutionize', 'industries', 'and', 'streamline'),\n",
       " ('to', 'revolutionize', 'industries', 'and', 'streamline', 'processes'),\n",
       " ('revolutionize', 'industries', 'and', 'streamline', 'processes', 'is'),\n",
       " ('industries', 'and', 'streamline', 'processes', 'is', 'immense'),\n",
       " ('and', 'streamline', 'processes', 'is', 'immense', ','),\n",
       " ('streamline', 'processes', 'is', 'immense', ',', 'promising'),\n",
       " ('processes', 'is', 'immense', ',', 'promising', 'greater'),\n",
       " ('is', 'immense', ',', 'promising', 'greater', 'efficiency'),\n",
       " ('immense', ',', 'promising', 'greater', 'efficiency', ','),\n",
       " (',', 'promising', 'greater', 'efficiency', ',', 'productivity'),\n",
       " ('promising', 'greater', 'efficiency', ',', 'productivity', ','),\n",
       " ('greater', 'efficiency', ',', 'productivity', ',', 'and'),\n",
       " ('efficiency', ',', 'productivity', ',', 'and', 'innovation'),\n",
       " (',', 'productivity', ',', 'and', 'innovation', '.'),\n",
       " ('productivity', ',', 'and', 'innovation', '.', 'However'),\n",
       " (',', 'and', 'innovation', '.', 'However', ','),\n",
       " ('and', 'innovation', '.', 'However', ',', 'concerns'),\n",
       " ('innovation', '.', 'However', ',', 'concerns', 'about'),\n",
       " ('.', 'However', ',', 'concerns', 'about', 'the'),\n",
       " ('However', ',', 'concerns', 'about', 'the', 'ethical'),\n",
       " (',', 'concerns', 'about', 'the', 'ethical', 'implications'),\n",
       " ('concerns', 'about', 'the', 'ethical', 'implications', 'of'),\n",
       " ('about', 'the', 'ethical', 'implications', 'of', 'AI'),\n",
       " ('the', 'ethical', 'implications', 'of', 'AI', ','),\n",
       " ('ethical', 'implications', 'of', 'AI', ',', 'such'),\n",
       " ('implications', 'of', 'AI', ',', 'such', 'as'),\n",
       " ('of', 'AI', ',', 'such', 'as', 'algorithmic'),\n",
       " ('AI', ',', 'such', 'as', 'algorithmic', 'bias'),\n",
       " (',', 'such', 'as', 'algorithmic', 'bias', 'and'),\n",
       " ('such', 'as', 'algorithmic', 'bias', 'and', 'privacy'),\n",
       " ('as', 'algorithmic', 'bias', 'and', 'privacy', 'breaches'),\n",
       " ('algorithmic', 'bias', 'and', 'privacy', 'breaches', ','),\n",
       " ('bias', 'and', 'privacy', 'breaches', ',', 'underscore'),\n",
       " ('and', 'privacy', 'breaches', ',', 'underscore', 'the'),\n",
       " ('privacy', 'breaches', ',', 'underscore', 'the', 'need'),\n",
       " ('breaches', ',', 'underscore', 'the', 'need', 'for'),\n",
       " (',', 'underscore', 'the', 'need', 'for', 'responsible'),\n",
       " ('underscore', 'the', 'need', 'for', 'responsible', 'development'),\n",
       " ('the', 'need', 'for', 'responsible', 'development', 'and'),\n",
       " ('need', 'for', 'responsible', 'development', 'and', 'deployment'),\n",
       " ('for', 'responsible', 'development', 'and', 'deployment', '.'),\n",
       " ('responsible', 'development', 'and', 'deployment', '.', 'Striking'),\n",
       " ('development', 'and', 'deployment', '.', 'Striking', 'a'),\n",
       " ('and', 'deployment', '.', 'Striking', 'a', 'balance'),\n",
       " ('deployment', '.', 'Striking', 'a', 'balance', 'between'),\n",
       " ('.', 'Striking', 'a', 'balance', 'between', 'harnessing'),\n",
       " ('Striking', 'a', 'balance', 'between', 'harnessing', 'the'),\n",
       " ('a', 'balance', 'between', 'harnessing', 'the', 'benefits'),\n",
       " ('balance', 'between', 'harnessing', 'the', 'benefits', 'of'),\n",
       " ('between', 'harnessing', 'the', 'benefits', 'of', 'AI'),\n",
       " ('harnessing', 'the', 'benefits', 'of', 'AI', 'and'),\n",
       " ('the', 'benefits', 'of', 'AI', 'and', 'addressing'),\n",
       " ('benefits', 'of', 'AI', 'and', 'addressing', 'its'),\n",
       " ('of', 'AI', 'and', 'addressing', 'its', 'societal'),\n",
       " ('AI', 'and', 'addressing', 'its', 'societal', 'impacts'),\n",
       " ('and', 'addressing', 'its', 'societal', 'impacts', 'is'),\n",
       " ('addressing', 'its', 'societal', 'impacts', 'is', 'essential'),\n",
       " ('its', 'societal', 'impacts', 'is', 'essential', 'for'),\n",
       " ('societal', 'impacts', 'is', 'essential', 'for', 'shaping'),\n",
       " ('impacts', 'is', 'essential', 'for', 'shaping', 'a'),\n",
       " ('is', 'essential', 'for', 'shaping', 'a', 'future'),\n",
       " ('essential', 'for', 'shaping', 'a', 'future', 'where'),\n",
       " ('for', 'shaping', 'a', 'future', 'where', 'AI'),\n",
       " ('shaping', 'a', 'future', 'where', 'AI', 'serves'),\n",
       " ('a', 'future', 'where', 'AI', 'serves', 'humanity'),\n",
       " ('future', 'where', 'AI', 'serves', 'humanity', 'best'),\n",
       " ('where', 'AI', 'serves', 'humanity', 'best', 'interests'),\n",
       " ('AI', 'serves', 'humanity', 'best', 'interests', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_Ngram = list(nltk.ngrams(quotes_tokens, 6))\n",
    "quotes_Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7582ba",
   "metadata": {},
   "source": [
    "# Stemming:\n",
    "\n",
    "### 1. PorterStemmer\n",
    "### 2. LancasterStemmer\n",
    "### 3. SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace9c4a",
   "metadata": {},
   "source": [
    "####  1. PorterStemmer => gives core word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f6d4ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affect'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. PorterStemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "pst= PorterStemmer()\n",
    "\n",
    "pst.stem('affection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fbf5b8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51c9e554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('STemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d2fce51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. From natural language processing to computer vision, AI applications span various domains, including healthcare, finance, and entertainment. With advancements in machine learning algorithms and computational power, AI systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nThe potential of AI to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. However, concerns about the ethical implications of AI, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. Striking a balance between harnessing the benefits of AI and addressing its societal impacts is essential for shaping a future where AI serves humanity best interests.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3143d3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence (ai) is a rapidly evolving field that encompasses a wide range of technologies aimed at mimicking human intelligence. from natural language processing to computer vision, ai applications span various domains, including healthcare, finance, and entertainment. with advancements in machine learning algorithms and computational power, ai systems are becoming increasingly capable of performing complex tasks autonomously.\\n\\nthe potential of ai to revolutionize industries and streamline processes is immense, promising greater efficiency, productivity, and innovation. however, concerns about the ethical implications of ai, such as algorithmic bias and privacy breaches, underscore the need for responsible development and deployment. striking a balance between harnessing the benefits of ai and addressing its societal impacts is essential for shaping a future where ai serves humanity best interests.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94674742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('give')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7740fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('giving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "866a5331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neha'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('neha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93c6b07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbdf107e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai/*'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('AI/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db6fd8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecfc1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "gaved : gave\n",
      "given : given\n",
      "gave : gave\n",
      "going : go\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + pst.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e88154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "gaved : gave\n",
      "given : given\n",
      "gave : gave\n",
      "going : go\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + pst.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7829949",
   "metadata": {},
   "source": [
    "#### 2. LancasterStemmer => gives ROOT core word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4bdfbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : giv\n",
      "giving : giv\n",
      "gaved : gav\n",
      "given : giv\n",
      "gave : gav\n",
      "going : going\n",
      "gone : gon\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + lst.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf4e49",
   "metadata": {},
   "source": [
    "#### 3. SnowballStemmer=> acts same like PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4747c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "gaved : gave\n",
      "given : given\n",
      "gave : gave\n",
      "going : go\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snbs = SnowballStemmer(language='english')\n",
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + snbs.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af53775",
   "metadata": {},
   "source": [
    "# Lemmatization:\n",
    "\n",
    "#### Gives full word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb502bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#word_net= wordnet()\n",
    "word_lem= WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34242060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : giving\n",
      "gaved : gaved\n",
      "given : given\n",
      "gave : gave\n",
      "going : going\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['give', 'giving',  'gaved', 'given', 'gave', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + word_lem.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68de30dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giv : giv\n",
      "giv : giv\n",
      "gav : gav\n",
      "giv : giv\n",
      "gav : gav\n",
      "going : going\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "words= ['giv', 'giv',  'gav', 'giv', 'gav', 'going', 'gone' ]\n",
    "for i in words:\n",
    "    print(i+ \" : \" + word_lem.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaf9f3",
   "metadata": {},
   "source": [
    "# StopWords:\n",
    "#### used for text cleaning, data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c408d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c0176e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'bcp47.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'extended_omw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw-1.4.zip', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pe08', 'pe08.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet.zip', 'wordnet2021.zip', 'wordnet2022', 'wordnet2022.zip', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0d37add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f05f8008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b6c01c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01',\n",
       " 'ca02',\n",
       " 'ca03',\n",
       " 'ca04',\n",
       " 'ca05',\n",
       " 'ca06',\n",
       " 'ca07',\n",
       " 'ca08',\n",
       " 'ca09',\n",
       " 'ca10',\n",
       " 'ca11',\n",
       " 'ca12',\n",
       " 'ca13',\n",
       " 'ca14',\n",
       " 'ca15',\n",
       " 'ca16',\n",
       " 'ca17',\n",
       " 'ca18',\n",
       " 'ca19',\n",
       " 'ca20',\n",
       " 'ca21',\n",
       " 'ca22',\n",
       " 'ca23',\n",
       " 'ca24',\n",
       " 'ca25',\n",
       " 'ca26',\n",
       " 'ca27',\n",
       " 'ca28',\n",
       " 'ca29',\n",
       " 'ca30',\n",
       " 'ca31',\n",
       " 'ca32',\n",
       " 'ca33',\n",
       " 'ca34',\n",
       " 'ca35',\n",
       " 'ca36',\n",
       " 'ca37',\n",
       " 'ca38',\n",
       " 'ca39',\n",
       " 'ca40',\n",
       " 'ca41',\n",
       " 'ca42',\n",
       " 'ca43',\n",
       " 'ca44',\n",
       " 'cb01',\n",
       " 'cb02',\n",
       " 'cb03',\n",
       " 'cb04',\n",
       " 'cb05',\n",
       " 'cb06',\n",
       " 'cb07',\n",
       " 'cb08',\n",
       " 'cb09',\n",
       " 'cb10',\n",
       " 'cb11',\n",
       " 'cb12',\n",
       " 'cb13',\n",
       " 'cb14',\n",
       " 'cb15',\n",
       " 'cb16',\n",
       " 'cb17',\n",
       " 'cb18',\n",
       " 'cb19',\n",
       " 'cb20',\n",
       " 'cb21',\n",
       " 'cb22',\n",
       " 'cb23',\n",
       " 'cb24',\n",
       " 'cb25',\n",
       " 'cb26',\n",
       " 'cb27',\n",
       " 'cc01',\n",
       " 'cc02',\n",
       " 'cc03',\n",
       " 'cc04',\n",
       " 'cc05',\n",
       " 'cc06',\n",
       " 'cc07',\n",
       " 'cc08',\n",
       " 'cc09',\n",
       " 'cc10',\n",
       " 'cc11',\n",
       " 'cc12',\n",
       " 'cc13',\n",
       " 'cc14',\n",
       " 'cc15',\n",
       " 'cc16',\n",
       " 'cc17',\n",
       " 'cd01',\n",
       " 'cd02',\n",
       " 'cd03',\n",
       " 'cd04',\n",
       " 'cd05',\n",
       " 'cd06',\n",
       " 'cd07',\n",
       " 'cd08',\n",
       " 'cd09',\n",
       " 'cd10',\n",
       " 'cd11',\n",
       " 'cd12',\n",
       " 'cd13',\n",
       " 'cd14',\n",
       " 'cd15',\n",
       " 'cd16',\n",
       " 'cd17',\n",
       " 'ce01',\n",
       " 'ce02',\n",
       " 'ce03',\n",
       " 'ce04',\n",
       " 'ce05',\n",
       " 'ce06',\n",
       " 'ce07',\n",
       " 'ce08',\n",
       " 'ce09',\n",
       " 'ce10',\n",
       " 'ce11',\n",
       " 'ce12',\n",
       " 'ce13',\n",
       " 'ce14',\n",
       " 'ce15',\n",
       " 'ce16',\n",
       " 'ce17',\n",
       " 'ce18',\n",
       " 'ce19',\n",
       " 'ce20',\n",
       " 'ce21',\n",
       " 'ce22',\n",
       " 'ce23',\n",
       " 'ce24',\n",
       " 'ce25',\n",
       " 'ce26',\n",
       " 'ce27',\n",
       " 'ce28',\n",
       " 'ce29',\n",
       " 'ce30',\n",
       " 'ce31',\n",
       " 'ce32',\n",
       " 'ce33',\n",
       " 'ce34',\n",
       " 'ce35',\n",
       " 'ce36',\n",
       " 'cf01',\n",
       " 'cf02',\n",
       " 'cf03',\n",
       " 'cf04',\n",
       " 'cf05',\n",
       " 'cf06',\n",
       " 'cf07',\n",
       " 'cf08',\n",
       " 'cf09',\n",
       " 'cf10',\n",
       " 'cf11',\n",
       " 'cf12',\n",
       " 'cf13',\n",
       " 'cf14',\n",
       " 'cf15',\n",
       " 'cf16',\n",
       " 'cf17',\n",
       " 'cf18',\n",
       " 'cf19',\n",
       " 'cf20',\n",
       " 'cf21',\n",
       " 'cf22',\n",
       " 'cf23',\n",
       " 'cf24',\n",
       " 'cf25',\n",
       " 'cf26',\n",
       " 'cf27',\n",
       " 'cf28',\n",
       " 'cf29',\n",
       " 'cf30',\n",
       " 'cf31',\n",
       " 'cf32',\n",
       " 'cf33',\n",
       " 'cf34',\n",
       " 'cf35',\n",
       " 'cf36',\n",
       " 'cf37',\n",
       " 'cf38',\n",
       " 'cf39',\n",
       " 'cf40',\n",
       " 'cf41',\n",
       " 'cf42',\n",
       " 'cf43',\n",
       " 'cf44',\n",
       " 'cf45',\n",
       " 'cf46',\n",
       " 'cf47',\n",
       " 'cf48',\n",
       " 'cg01',\n",
       " 'cg02',\n",
       " 'cg03',\n",
       " 'cg04',\n",
       " 'cg05',\n",
       " 'cg06',\n",
       " 'cg07',\n",
       " 'cg08',\n",
       " 'cg09',\n",
       " 'cg10',\n",
       " 'cg11',\n",
       " 'cg12',\n",
       " 'cg13',\n",
       " 'cg14',\n",
       " 'cg15',\n",
       " 'cg16',\n",
       " 'cg17',\n",
       " 'cg18',\n",
       " 'cg19',\n",
       " 'cg20',\n",
       " 'cg21',\n",
       " 'cg22',\n",
       " 'cg23',\n",
       " 'cg24',\n",
       " 'cg25',\n",
       " 'cg26',\n",
       " 'cg27',\n",
       " 'cg28',\n",
       " 'cg29',\n",
       " 'cg30',\n",
       " 'cg31',\n",
       " 'cg32',\n",
       " 'cg33',\n",
       " 'cg34',\n",
       " 'cg35',\n",
       " 'cg36',\n",
       " 'cg37',\n",
       " 'cg38',\n",
       " 'cg39',\n",
       " 'cg40',\n",
       " 'cg41',\n",
       " 'cg42',\n",
       " 'cg43',\n",
       " 'cg44',\n",
       " 'cg45',\n",
       " 'cg46',\n",
       " 'cg47',\n",
       " 'cg48',\n",
       " 'cg49',\n",
       " 'cg50',\n",
       " 'cg51',\n",
       " 'cg52',\n",
       " 'cg53',\n",
       " 'cg54',\n",
       " 'cg55',\n",
       " 'cg56',\n",
       " 'cg57',\n",
       " 'cg58',\n",
       " 'cg59',\n",
       " 'cg60',\n",
       " 'cg61',\n",
       " 'cg62',\n",
       " 'cg63',\n",
       " 'cg64',\n",
       " 'cg65',\n",
       " 'cg66',\n",
       " 'cg67',\n",
       " 'cg68',\n",
       " 'cg69',\n",
       " 'cg70',\n",
       " 'cg71',\n",
       " 'cg72',\n",
       " 'cg73',\n",
       " 'cg74',\n",
       " 'cg75',\n",
       " 'ch01',\n",
       " 'ch02',\n",
       " 'ch03',\n",
       " 'ch04',\n",
       " 'ch05',\n",
       " 'ch06',\n",
       " 'ch07',\n",
       " 'ch08',\n",
       " 'ch09',\n",
       " 'ch10',\n",
       " 'ch11',\n",
       " 'ch12',\n",
       " 'ch13',\n",
       " 'ch14',\n",
       " 'ch15',\n",
       " 'ch16',\n",
       " 'ch17',\n",
       " 'ch18',\n",
       " 'ch19',\n",
       " 'ch20',\n",
       " 'ch21',\n",
       " 'ch22',\n",
       " 'ch23',\n",
       " 'ch24',\n",
       " 'ch25',\n",
       " 'ch26',\n",
       " 'ch27',\n",
       " 'ch28',\n",
       " 'ch29',\n",
       " 'ch30',\n",
       " 'cj01',\n",
       " 'cj02',\n",
       " 'cj03',\n",
       " 'cj04',\n",
       " 'cj05',\n",
       " 'cj06',\n",
       " 'cj07',\n",
       " 'cj08',\n",
       " 'cj09',\n",
       " 'cj10',\n",
       " 'cj11',\n",
       " 'cj12',\n",
       " 'cj13',\n",
       " 'cj14',\n",
       " 'cj15',\n",
       " 'cj16',\n",
       " 'cj17',\n",
       " 'cj18',\n",
       " 'cj19',\n",
       " 'cj20',\n",
       " 'cj21',\n",
       " 'cj22',\n",
       " 'cj23',\n",
       " 'cj24',\n",
       " 'cj25',\n",
       " 'cj26',\n",
       " 'cj27',\n",
       " 'cj28',\n",
       " 'cj29',\n",
       " 'cj30',\n",
       " 'cj31',\n",
       " 'cj32',\n",
       " 'cj33',\n",
       " 'cj34',\n",
       " 'cj35',\n",
       " 'cj36',\n",
       " 'cj37',\n",
       " 'cj38',\n",
       " 'cj39',\n",
       " 'cj40',\n",
       " 'cj41',\n",
       " 'cj42',\n",
       " 'cj43',\n",
       " 'cj44',\n",
       " 'cj45',\n",
       " 'cj46',\n",
       " 'cj47',\n",
       " 'cj48',\n",
       " 'cj49',\n",
       " 'cj50',\n",
       " 'cj51',\n",
       " 'cj52',\n",
       " 'cj53',\n",
       " 'cj54',\n",
       " 'cj55',\n",
       " 'cj56',\n",
       " 'cj57',\n",
       " 'cj58',\n",
       " 'cj59',\n",
       " 'cj60',\n",
       " 'cj61',\n",
       " 'cj62',\n",
       " 'cj63',\n",
       " 'cj64',\n",
       " 'cj65',\n",
       " 'cj66',\n",
       " 'cj67',\n",
       " 'cj68',\n",
       " 'cj69',\n",
       " 'cj70',\n",
       " 'cj71',\n",
       " 'cj72',\n",
       " 'cj73',\n",
       " 'cj74',\n",
       " 'cj75',\n",
       " 'cj76',\n",
       " 'cj77',\n",
       " 'cj78',\n",
       " 'cj79',\n",
       " 'cj80',\n",
       " 'ck01',\n",
       " 'ck02',\n",
       " 'ck03',\n",
       " 'ck04',\n",
       " 'ck05',\n",
       " 'ck06',\n",
       " 'ck07',\n",
       " 'ck08',\n",
       " 'ck09',\n",
       " 'ck10',\n",
       " 'ck11',\n",
       " 'ck12',\n",
       " 'ck13',\n",
       " 'ck14',\n",
       " 'ck15',\n",
       " 'ck16',\n",
       " 'ck17',\n",
       " 'ck18',\n",
       " 'ck19',\n",
       " 'ck20',\n",
       " 'ck21',\n",
       " 'ck22',\n",
       " 'ck23',\n",
       " 'ck24',\n",
       " 'ck25',\n",
       " 'ck26',\n",
       " 'ck27',\n",
       " 'ck28',\n",
       " 'ck29',\n",
       " 'cl01',\n",
       " 'cl02',\n",
       " 'cl03',\n",
       " 'cl04',\n",
       " 'cl05',\n",
       " 'cl06',\n",
       " 'cl07',\n",
       " 'cl08',\n",
       " 'cl09',\n",
       " 'cl10',\n",
       " 'cl11',\n",
       " 'cl12',\n",
       " 'cl13',\n",
       " 'cl14',\n",
       " 'cl15',\n",
       " 'cl16',\n",
       " 'cl17',\n",
       " 'cl18',\n",
       " 'cl19',\n",
       " 'cl20',\n",
       " 'cl21',\n",
       " 'cl22',\n",
       " 'cl23',\n",
       " 'cl24',\n",
       " 'cm01',\n",
       " 'cm02',\n",
       " 'cm03',\n",
       " 'cm04',\n",
       " 'cm05',\n",
       " 'cm06',\n",
       " 'cn01',\n",
       " 'cn02',\n",
       " 'cn03',\n",
       " 'cn04',\n",
       " 'cn05',\n",
       " 'cn06',\n",
       " 'cn07',\n",
       " 'cn08',\n",
       " 'cn09',\n",
       " 'cn10',\n",
       " 'cn11',\n",
       " 'cn12',\n",
       " 'cn13',\n",
       " 'cn14',\n",
       " 'cn15',\n",
       " 'cn16',\n",
       " 'cn17',\n",
       " 'cn18',\n",
       " 'cn19',\n",
       " 'cn20',\n",
       " 'cn21',\n",
       " 'cn22',\n",
       " 'cn23',\n",
       " 'cn24',\n",
       " 'cn25',\n",
       " 'cn26',\n",
       " 'cn27',\n",
       " 'cn28',\n",
       " 'cn29',\n",
       " 'cp01',\n",
       " 'cp02',\n",
       " 'cp03',\n",
       " 'cp04',\n",
       " 'cp05',\n",
       " 'cp06',\n",
       " 'cp07',\n",
       " 'cp08',\n",
       " 'cp09',\n",
       " 'cp10',\n",
       " 'cp11',\n",
       " 'cp12',\n",
       " 'cp13',\n",
       " 'cp14',\n",
       " 'cp15',\n",
       " 'cp16',\n",
       " 'cp17',\n",
       " 'cp18',\n",
       " 'cp19',\n",
       " 'cp20',\n",
       " 'cp21',\n",
       " 'cp22',\n",
       " 'cp23',\n",
       " 'cp24',\n",
       " 'cp25',\n",
       " 'cp26',\n",
       " 'cp27',\n",
       " 'cp28',\n",
       " 'cp29',\n",
       " 'cr01',\n",
       " 'cr02',\n",
       " 'cr03',\n",
       " 'cr04',\n",
       " 'cr05',\n",
       " 'cr06',\n",
       " 'cr07',\n",
       " 'cr08',\n",
       " 'cr09']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6eb5ea0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8613746f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001375de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "833cb05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一',\n",
       " '一下',\n",
       " '一些',\n",
       " '一切',\n",
       " '一则',\n",
       " '一天',\n",
       " '一定',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一时',\n",
       " '一来',\n",
       " '一样',\n",
       " '一次',\n",
       " '一片',\n",
       " '一直',\n",
       " '一致',\n",
       " '一般',\n",
       " '一起',\n",
       " '一边',\n",
       " '一面',\n",
       " '万一',\n",
       " '上下',\n",
       " '上升',\n",
       " '上去',\n",
       " '上来',\n",
       " '上述',\n",
       " '上面',\n",
       " '下列',\n",
       " '下去',\n",
       " '下来',\n",
       " '下面',\n",
       " '不一',\n",
       " '不久',\n",
       " '不仅',\n",
       " '不会',\n",
       " '不但',\n",
       " '不光',\n",
       " '不单',\n",
       " '不变',\n",
       " '不只',\n",
       " '不可',\n",
       " '不同',\n",
       " '不够',\n",
       " '不如',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不敢',\n",
       " '不断',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不特',\n",
       " '不独',\n",
       " '不管',\n",
       " '不能',\n",
       " '不要',\n",
       " '不论',\n",
       " '不足',\n",
       " '不过',\n",
       " '不问',\n",
       " '与',\n",
       " '与其',\n",
       " '与否',\n",
       " '与此同时',\n",
       " '专门',\n",
       " '且',\n",
       " '两者',\n",
       " '严格',\n",
       " '严重',\n",
       " '个',\n",
       " '个人',\n",
       " '个别',\n",
       " '中小',\n",
       " '中间',\n",
       " '丰富',\n",
       " '临',\n",
       " '为',\n",
       " '为主',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为什麽',\n",
       " '为何',\n",
       " '为着',\n",
       " '主张',\n",
       " '主要',\n",
       " '举行',\n",
       " '乃',\n",
       " '乃至',\n",
       " '么',\n",
       " '之',\n",
       " '之一',\n",
       " '之前',\n",
       " '之后',\n",
       " '之後',\n",
       " '之所以',\n",
       " '之类',\n",
       " '乌乎',\n",
       " '乎',\n",
       " '乘',\n",
       " '也',\n",
       " '也好',\n",
       " '也是',\n",
       " '也罢',\n",
       " '了',\n",
       " '了解',\n",
       " '争取',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '云云',\n",
       " '互相',\n",
       " '产生',\n",
       " '人们',\n",
       " '人家',\n",
       " '什么',\n",
       " '什么样',\n",
       " '什麽',\n",
       " '今后',\n",
       " '今天',\n",
       " '今年',\n",
       " '今後',\n",
       " '仍然',\n",
       " '从',\n",
       " '从事',\n",
       " '从而',\n",
       " '他',\n",
       " '他人',\n",
       " '他们',\n",
       " '他的',\n",
       " '代替',\n",
       " '以',\n",
       " '以上',\n",
       " '以下',\n",
       " '以为',\n",
       " '以便',\n",
       " '以免',\n",
       " '以前',\n",
       " '以及',\n",
       " '以后',\n",
       " '以外',\n",
       " '以後',\n",
       " '以来',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '们',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '任务',\n",
       " '企图',\n",
       " '伟大',\n",
       " '似乎',\n",
       " '似的',\n",
       " '但',\n",
       " '但是',\n",
       " '何',\n",
       " '何况',\n",
       " '何处',\n",
       " '何时',\n",
       " '作为',\n",
       " '你',\n",
       " '你们',\n",
       " '你的',\n",
       " '使得',\n",
       " '使用',\n",
       " '例如',\n",
       " '依',\n",
       " '依照',\n",
       " '依靠',\n",
       " '促进',\n",
       " '保持',\n",
       " '俺',\n",
       " '俺们',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '做到',\n",
       " '像',\n",
       " '允许',\n",
       " '充分',\n",
       " '先后',\n",
       " '先後',\n",
       " '先生',\n",
       " '全部',\n",
       " '全面',\n",
       " '兮',\n",
       " '共同',\n",
       " '关于',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其余',\n",
       " '其它',\n",
       " '其实',\n",
       " '其次',\n",
       " '具体',\n",
       " '具体地说',\n",
       " '具体说来',\n",
       " '具有',\n",
       " '再者',\n",
       " '再说',\n",
       " '冒',\n",
       " '冲',\n",
       " '决定',\n",
       " '况且',\n",
       " '准备',\n",
       " '几',\n",
       " '几乎',\n",
       " '几时',\n",
       " '凭',\n",
       " '凭借',\n",
       " '出去',\n",
       " '出来',\n",
       " '出现',\n",
       " '分别',\n",
       " '则',\n",
       " '别',\n",
       " '别的',\n",
       " '别说',\n",
       " '到',\n",
       " '前后',\n",
       " '前者',\n",
       " '前进',\n",
       " '前面',\n",
       " '加之',\n",
       " '加以',\n",
       " '加入',\n",
       " '加强',\n",
       " '十分',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即或',\n",
       " '即若',\n",
       " '却不',\n",
       " '原来',\n",
       " '又',\n",
       " '及',\n",
       " '及其',\n",
       " '及时',\n",
       " '及至',\n",
       " '双方',\n",
       " '反之',\n",
       " '反应',\n",
       " '反映',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '取得',\n",
       " '受到',\n",
       " '变成',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '只是',\n",
       " '只有',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叫做',\n",
       " '召开',\n",
       " '叮咚',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可能',\n",
       " '可见',\n",
       " '各',\n",
       " '各个',\n",
       " '各人',\n",
       " '各位',\n",
       " '各地',\n",
       " '各种',\n",
       " '各级',\n",
       " '各自',\n",
       " '合理',\n",
       " '同',\n",
       " '同一',\n",
       " '同时',\n",
       " '同样',\n",
       " '后来',\n",
       " '后面',\n",
       " '向',\n",
       " '向着',\n",
       " '吓',\n",
       " '吗',\n",
       " '否则',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '呕',\n",
       " '呗',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '呢',\n",
       " '周围',\n",
       " '呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咱',\n",
       " '咱们',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '哗',\n",
       " '哟',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪个',\n",
       " '哪些',\n",
       " '哪儿',\n",
       " '哪天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪样',\n",
       " '哪边',\n",
       " '哪里',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '啊',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪达',\n",
       " '喂',\n",
       " '喏',\n",
       " '喔唷',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '嘘',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '因',\n",
       " '因为',\n",
       " '因此',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在下',\n",
       " '地',\n",
       " '坚决',\n",
       " '坚持',\n",
       " '基本',\n",
       " '处理',\n",
       " '复杂',\n",
       " '多',\n",
       " '多少',\n",
       " '多数',\n",
       " '多次',\n",
       " '大力',\n",
       " '大多数',\n",
       " '大大',\n",
       " '大家',\n",
       " '大批',\n",
       " '大约',\n",
       " '大量',\n",
       " '失去',\n",
       " '她',\n",
       " '她们',\n",
       " '她的',\n",
       " '好的',\n",
       " '好象',\n",
       " '如',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如何',\n",
       " '如其',\n",
       " '如果',\n",
       " '如此',\n",
       " '如若',\n",
       " '存在',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁愿',\n",
       " '宁肯',\n",
       " '它',\n",
       " '它们',\n",
       " '它们的',\n",
       " '它的',\n",
       " '安全',\n",
       " '完全',\n",
       " '完成',\n",
       " '实现',\n",
       " '实际',\n",
       " '宣布',\n",
       " '容易',\n",
       " '密切',\n",
       " '对',\n",
       " '对于',\n",
       " '对应',\n",
       " '将',\n",
       " '少数',\n",
       " '尔后',\n",
       " '尚且',\n",
       " '尤其',\n",
       " '就',\n",
       " '就是',\n",
       " '就是说',\n",
       " '尽',\n",
       " '尽管',\n",
       " '属于',\n",
       " '岂但',\n",
       " '左右',\n",
       " '巨大',\n",
       " '巩固',\n",
       " '己',\n",
       " '已经',\n",
       " '帮助',\n",
       " '常常',\n",
       " '并',\n",
       " '并不',\n",
       " '并不是',\n",
       " '并且',\n",
       " '并没有',\n",
       " '广大',\n",
       " '广泛',\n",
       " '应当',\n",
       " '应用',\n",
       " '应该',\n",
       " '开外',\n",
       " '开始',\n",
       " '开展',\n",
       " '引起',\n",
       " '强烈',\n",
       " '强调',\n",
       " '归',\n",
       " '当',\n",
       " '当前',\n",
       " '当时',\n",
       " '当然',\n",
       " '当着',\n",
       " '形成',\n",
       " '彻底',\n",
       " '彼',\n",
       " '彼此',\n",
       " '往',\n",
       " '往往',\n",
       " '待',\n",
       " '後来',\n",
       " '後面',\n",
       " '得',\n",
       " '得出',\n",
       " '得到',\n",
       " '心里',\n",
       " '必然',\n",
       " '必要',\n",
       " '必须',\n",
       " '怎',\n",
       " '怎么',\n",
       " '怎么办',\n",
       " '怎么样',\n",
       " '怎样',\n",
       " '怎麽',\n",
       " '总之',\n",
       " '总是',\n",
       " '总的来看',\n",
       " '总的来说',\n",
       " '总的说来',\n",
       " '总结',\n",
       " '总而言之',\n",
       " '恰恰相反',\n",
       " '您',\n",
       " '意思',\n",
       " '愿意',\n",
       " '慢说',\n",
       " '成为',\n",
       " '我',\n",
       " '我们',\n",
       " '我的',\n",
       " '或',\n",
       " '或是',\n",
       " '或者',\n",
       " '战斗',\n",
       " '所',\n",
       " '所以',\n",
       " '所有',\n",
       " '所谓',\n",
       " '打',\n",
       " '扩大',\n",
       " '把',\n",
       " '抑或',\n",
       " '拿',\n",
       " '按',\n",
       " '按照',\n",
       " '换句话说',\n",
       " '换言之',\n",
       " '据',\n",
       " '掌握',\n",
       " '接着',\n",
       " '接著',\n",
       " '故',\n",
       " '故此',\n",
       " '整个',\n",
       " '方便',\n",
       " '方面',\n",
       " '旁人',\n",
       " '无宁',\n",
       " '无法',\n",
       " '无论',\n",
       " '既',\n",
       " '既是',\n",
       " '既然',\n",
       " '时候',\n",
       " '明显',\n",
       " '明确',\n",
       " '是',\n",
       " '是否',\n",
       " '是的',\n",
       " '显然',\n",
       " '显著',\n",
       " '普通',\n",
       " '普遍',\n",
       " '更加',\n",
       " '曾经',\n",
       " '替',\n",
       " '最后',\n",
       " '最大',\n",
       " '最好',\n",
       " '最後',\n",
       " '最近',\n",
       " '最高',\n",
       " '有',\n",
       " '有些',\n",
       " '有关',\n",
       " '有利',\n",
       " '有力',\n",
       " '有所',\n",
       " '有效',\n",
       " '有时',\n",
       " '有点',\n",
       " '有的',\n",
       " '有着',\n",
       " '有著',\n",
       " '望',\n",
       " '朝',\n",
       " '朝着',\n",
       " '本',\n",
       " '本着',\n",
       " '来',\n",
       " '来着',\n",
       " '极了',\n",
       " '构成',\n",
       " '果然',\n",
       " '果真',\n",
       " '某',\n",
       " '某个',\n",
       " '某些',\n",
       " '根据',\n",
       " '根本',\n",
       " '欢迎',\n",
       " '正在',\n",
       " '正如',\n",
       " '正常',\n",
       " '此',\n",
       " '此外',\n",
       " '此时',\n",
       " '此间',\n",
       " '毋宁',\n",
       " '每',\n",
       " '每个',\n",
       " '每天',\n",
       " '每年',\n",
       " '每当',\n",
       " '比',\n",
       " '比如',\n",
       " '比方',\n",
       " '比较',\n",
       " '毫不',\n",
       " '没有',\n",
       " '沿',\n",
       " '沿着',\n",
       " '注意',\n",
       " '深入',\n",
       " '清楚',\n",
       " '满足',\n",
       " '漫说',\n",
       " '焉',\n",
       " '然则',\n",
       " '然后',\n",
       " '然後',\n",
       " '然而',\n",
       " '照',\n",
       " '照着',\n",
       " '特别是',\n",
       " '特殊',\n",
       " '特点',\n",
       " '现代',\n",
       " '现在',\n",
       " '甚么',\n",
       " '甚而',\n",
       " '甚至',\n",
       " '用',\n",
       " '由',\n",
       " '由于',\n",
       " '由此可见',\n",
       " '的',\n",
       " '的话',\n",
       " '目前',\n",
       " '直到',\n",
       " '直接',\n",
       " '相似',\n",
       " '相信',\n",
       " '相反',\n",
       " '相同',\n",
       " '相对',\n",
       " '相对而言',\n",
       " '相应',\n",
       " '相当',\n",
       " '相等',\n",
       " '省得',\n",
       " '看出',\n",
       " '看到',\n",
       " '看来',\n",
       " '看看',\n",
       " '看见',\n",
       " '真是',\n",
       " '真正',\n",
       " '着',\n",
       " '着呢',\n",
       " '矣',\n",
       " '知道',\n",
       " '确定',\n",
       " '离',\n",
       " '积极',\n",
       " '移动',\n",
       " '突出',\n",
       " '突然',\n",
       " '立即',\n",
       " '第',\n",
       " '等',\n",
       " '等等',\n",
       " '管',\n",
       " '紧接着',\n",
       " '纵',\n",
       " '纵令',\n",
       " '纵使',\n",
       " '纵然',\n",
       " '练习',\n",
       " '组成',\n",
       " '经',\n",
       " '经常',\n",
       " '经过',\n",
       " '结合',\n",
       " '结果',\n",
       " '给',\n",
       " '绝对',\n",
       " '继续',\n",
       " '继而',\n",
       " '维持',\n",
       " '综上所述',\n",
       " '罢了',\n",
       " '考虑',\n",
       " '者',\n",
       " '而',\n",
       " '而且',\n",
       " '而况',\n",
       " '而外',\n",
       " '而已',\n",
       " '而是',\n",
       " '而言',\n",
       " '联系',\n",
       " '能',\n",
       " '能否',\n",
       " '能够',\n",
       " '腾',\n",
       " '自',\n",
       " '自个儿',\n",
       " '自从',\n",
       " '自各儿',\n",
       " '自家',\n",
       " '自己',\n",
       " '自身',\n",
       " '至',\n",
       " '至于',\n",
       " '良好',\n",
       " '若',\n",
       " '若是',\n",
       " '若非',\n",
       " '范围',\n",
       " '莫若',\n",
       " '获得',\n",
       " '虽',\n",
       " '虽则',\n",
       " '虽然',\n",
       " '虽说',\n",
       " '行为',\n",
       " '行动',\n",
       " '表明',\n",
       " '表示',\n",
       " '被',\n",
       " '要',\n",
       " '要不',\n",
       " '要不是',\n",
       " '要不然',\n",
       " '要么',\n",
       " '要是',\n",
       " '要求',\n",
       " '规定',\n",
       " '觉得',\n",
       " '认为',\n",
       " '认真',\n",
       " '认识',\n",
       " '让',\n",
       " '许多',\n",
       " '论',\n",
       " '设使',\n",
       " '设若',\n",
       " '该',\n",
       " '说明',\n",
       " '诸位',\n",
       " '谁',\n",
       " '谁知',\n",
       " '赶',\n",
       " '起',\n",
       " '起来',\n",
       " '起见',\n",
       " '趁',\n",
       " '趁着',\n",
       " '越是',\n",
       " '跟',\n",
       " '转动',\n",
       " '转变',\n",
       " '转贴',\n",
       " '较',\n",
       " '较之',\n",
       " '边',\n",
       " '达到',\n",
       " '迅速',\n",
       " '过',\n",
       " '过去',\n",
       " '过来',\n",
       " '运用',\n",
       " '还是',\n",
       " '还有',\n",
       " '这',\n",
       " '这个',\n",
       " '这么',\n",
       " '这么些',\n",
       " '这么样',\n",
       " '这么点儿',\n",
       " '这些',\n",
       " '这会儿',\n",
       " '这儿',\n",
       " '这就是说',\n",
       " '这时',\n",
       " '这样',\n",
       " '这点',\n",
       " '这种',\n",
       " '这边',\n",
       " '这里',\n",
       " '这麽',\n",
       " '进入',\n",
       " '进步',\n",
       " '进而',\n",
       " '进行',\n",
       " '连',\n",
       " '连同',\n",
       " '适应',\n",
       " '适当',\n",
       " '适用',\n",
       " '逐步',\n",
       " '逐渐',\n",
       " '通常',\n",
       " '通过',\n",
       " '造成',\n",
       " '遇到',\n",
       " '遭到',\n",
       " '避免',\n",
       " '那',\n",
       " '那个',\n",
       " '那么',\n",
       " '那么些',\n",
       " '那么样',\n",
       " '那些',\n",
       " '那会儿',\n",
       " '那儿',\n",
       " '那时',\n",
       " '那样',\n",
       " '那边',\n",
       " '那里',\n",
       " '那麽',\n",
       " '部分',\n",
       " '鄙人',\n",
       " '采取',\n",
       " '里面',\n",
       " '重大',\n",
       " '重新',\n",
       " '重要',\n",
       " '鉴于',\n",
       " '问题',\n",
       " '防止',\n",
       " '阿',\n",
       " '附近',\n",
       " '限制',\n",
       " '除',\n",
       " '除了',\n",
       " '除此之外',\n",
       " '除非',\n",
       " '随',\n",
       " '随着',\n",
       " '随著',\n",
       " '集中',\n",
       " '需要',\n",
       " '非但',\n",
       " '非常',\n",
       " '非徒',\n",
       " '靠',\n",
       " '顺',\n",
       " '顺着',\n",
       " '首先',\n",
       " '高兴',\n",
       " '是不是']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84c89564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ebb5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\Neha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\hindi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhindi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39mjoin(file)\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\Neha\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\hindi'"
     ]
    }
   ],
   "source": [
    "stopwords.words('hindi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e18a88",
   "metadata": {},
   "source": [
    "# As we see there is no inbuilt stopwords lib for hindi but  I have made a project for hindi stopwords also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030b90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812723b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
